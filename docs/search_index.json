[["index.html", "Introduction to Optimization Preface", " Introduction to Optimization Apurva Nakade 2022-05-23 Preface "],["introduction.html", "Chapter 1 Introduction 1.1 Software tools", " Chapter 1 Introduction The question of optimization is the very general question of deciding when a function \\(f(x_1, \\dots, x_n)\\) attains its maximum or minimum value on a domain \\(D\\) in \\(\\mathbb{R^n}\\). \\[\\begin{align} \\mbox{optimize: } &amp;&amp; f(x_1, \\dots, x_n) &amp; \\\\ \\mbox{subject to: } &amp;&amp; (x_1, \\dots, x_n) &amp;\\in D. \\end{align}\\] These kinds questions show up naturally in every quantitative field. To analyze the question meaningfully, one needs to make some assumptions on \\(f\\) and \\(D\\). In this course, we’ll analyze this question in the special case when the function \\(f\\) is linear and the constraint set \\(D\\) is described using linear inequalities. The study of this problem is called Linear Programming. Despite the simplicity of the Linear Programming setup, or perhaps because of it, LP is one of the most commonly used models for optimization problems. We’ll see how to solve linear programs using the simplex method and go on to analyze the solution sets using duality theory. Linear programs are used for modeling real world problems when prices/costs and constraints are fixed and known beforehand. Even when this is not the case, linear programs are often used to approximate and estimate costs/prices before moving on to more sophisticated techniques. Example 1.1 A bond portfolio manager has $100,000 to allocate to two different bonds; one corporate and one government bond. The corporate bond has a yield of 4%, a maturity of 3 years and an A rating from a rating agency that is translated into a numerical rating of 2 for computational purposes. In contrast, the government bond has a yield of 3%, a maturity of 6 years and rating of Aaa with the corresponding numerical rating of 1 (lower numerical ratings correspond to higher quality bonds). The portfolio manager would like to allocate her funds so that the average rating for the portfolio is no worse than Aa (numerical equivalent 1.5) and average maturity of the portfolio is at most 3.6 years. Any amount not invested in the two bonds will be kept in a cash account that is assumed to earn no interest for simplicity and does not contribute to the average rating or maturity computations. How should the manager allocate her funds between these two bonds to achieve her objective of maximizing the annual yield from this investment? (Cornuéjols, Peña, and Tütüncü 2018) Corporate Government Bounds Yield 4% 3% Maturity 3 6 3.6 Rating A = 2 Aaa = 1 1.5 Fund Allocations ?? ?? 100,000 Solution We can model the above problem as follows: \\[\\begin{equation} \\begin{array}{rrrrrr} \\mbox{maximize:} &amp; 4x &amp; + &amp; 3y \\\\ \\mbox{subject to:} &amp; 3x &amp; + &amp; 6y &amp; \\le &amp; 3.6 \\\\ &amp; 2x &amp; + &amp; y &amp; \\le &amp; 1.5 % &amp; x &amp; + &amp; y &amp; \\le &amp; 1 \\\\ % &amp; x &amp; , &amp; y &amp; \\ge &amp; 0, \\end{array} \\tag{1.1} \\end{equation}\\] where \\(x\\), \\(y\\) are the percentages of corporate and government bonds, respectively, and the objective function when multiplied by $100,000 gives us the net yield. This is an example of a linear program. Note that we cannot subtract inequalities the same way that we can subtract equalities. So to get started, let us assume that both the inequalities are in fact equalities. We can solve the system \\[\\begin{equation} \\begin{array}{rrrrrl} &amp; 3x &amp; + &amp; 6y &amp; = &amp; 3.6 \\\\ &amp; 2x &amp; + &amp; y &amp; = &amp; 1.5 \\end{array} \\end{equation}\\] to obtain \\(x = 0.6\\) and \\(y = 0.3\\). But for this solution \\(x + y = 0.9\\) which is less that 1, meaning that we’re not investing all the available funds! Which raises the question: Is it possible to increase the yield further by not satisfying both of the above equalities and investing all the money instead? This question becomes even more apparent once we realize that the linear program (1.1) is incomplete and the complete linear program that models the problem is as follows: \\[\\begin{equation} \\begin{array}{rrrrrl} \\mbox{maximize:} &amp; 4x &amp; + &amp; 3y \\\\ \\mbox{subject to:} &amp; 3x &amp; + &amp; 6y &amp; \\le &amp; 3.6 \\\\ &amp; 2x &amp; + &amp; y &amp; \\le &amp; 1.5 \\\\ &amp; x &amp; + &amp; y &amp; \\le &amp; 1 \\\\ &amp; x &amp; &amp; &amp; \\ge &amp; 0 \\\\ &amp; &amp; &amp; y &amp; \\ge &amp; 0. \\end{array} \\tag{1.2} \\end{equation}\\] The solution \\((x, y) = (0.6, 0.3)\\) is obtained by changing the first two inequalities to equalities. But this choice is completely arbitrary! We could have switched some other set of inequalities to equalities and obtained a different solution. We’d then need to compare the solutions obtained in each of these cases and find the one that maximizes it. This method becomes unwieldy very fast and we’ll need to develop better ways to solve linear programs. 1.1 Software tools 1.1.1 Graphing calculator Linear programs in two variables can be visualized using a graphing calculator. The constraints in Example 1.1 can be visualized as follows: The feasible set is the quadrilateral formed by the overlap of all three constraint regions. The level sets of the objective function are straight lines of the form \\(4x + 3y = c\\). We can use a graphical calculator to find the largest value of \\(c\\) for which these level sets intersect the feasible region, which turns out to be \\(c = 3.3\\%\\) which gives us a net yield of \\(3.3\\% \\times \\$100,000 = \\$3,300\\). Here is the same picture in Desmos: https://www.desmos.com/calculator/bexwrrcbwx As it turns out, there is no efficient way to implement this method purely algebraically, thereby making it unusable in higher dimensions. 1.1.2 Solver add-in Excel and Google sheets have free, easy-to-use linear program solvers that can be used to solve linear programs and generate sensitivity analysis. Here is a sample solution of Example 1.1 using Solver in Excel: https://1drv.ms/x/s!AnwQOvs0HXuihl3PWf5YQLWsF6qB?e=rddRBy References "],["standard-linear-program.html", "Chapter 2 Standard linear program 2.1 Slack variables 2.2 Basic and non-basic variables", " Chapter 2 Standard linear program A standard linear program is an optimization problem of the following form: \\[\\begin{equation} \\begin{array}{lrrrrrrrrr} \\mbox{maximize: } &amp; c_0 &amp; + &amp; c_1 x_1 &amp; + &amp; \\dots &amp; + &amp; c_n x_n &amp; \\\\ \\mbox{subject to: } &amp; &amp; &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; \\leq &amp; b_1 \\\\ &amp; &amp; &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; \\leq &amp; b_2 \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ &amp; &amp; &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; \\leq &amp; b_m \\\\ &amp; &amp; &amp; x_1, &amp; x_2, &amp; \\dots &amp;, &amp; x_n &amp; \\geq &amp; 0 \\end{array} \\tag{2.1} \\end{equation}\\] where \\(c_i\\), \\(a_{ij}\\), and \\(b_j\\) are real constants. The variables \\(x_1, \\dots, x_n\\) are called decision variables. The set of tuples \\((x_1, \\dots, x_n)\\) that satisfy all the constraints is called the feasible region. Example 2.1 Equation (1.2) is an example of a standard linear program with 2 decision variables, 3 constraints, and the feasible region being a quadrilateral. Remark. Not every linear program is standard. However, we will see later that every linear program can be standardized and hence it suffices to construct an algorithm for solving standard linear programs. We’ll assume the following two theorems without proof for now: Theorem 2.1 The feasible region of a standard linear program is either empty, or a convex polytope (possibly degenerate or infinite) . The precise definition of a convex polytope is quite complicated. For the purposes of this class, it is sufficient to think of a polytope as a region that has vertices and edges. Theorem 2.2 Every standard linear program attains its optimal solution, if any, at one of the vertices of the feasible region. Note that the above theorem claims neither the existence nor the uniqueness of an optimal solution. All it is saying is that if a maximum objective value exists then it is attained at one of the vertices. It is possible that no optimal value exists or that the optimal value is attained more than one points, possible at a non-vertex. 2.1 Slack variables For each constraint, we introduce a slack variable by subtracting the LHS from the RHS as follows. \\[\\begin{equation} \\begin{array}{lrrrrrrrrr} w_1 &amp; = &amp; b_1 &amp; - &amp; a_{11} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{1n} x_n \\\\ w_2 &amp; = &amp; b_2 &amp; - &amp; a_{21} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{2n} x_n \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ w_m &amp; = &amp; b_m &amp; - &amp; a_{m1} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{mn} x_n \\end{array} \\tag{2.2} \\end{equation}\\] We can think of the slack variable \\(w_i\\) as measuring the “slackness” in the \\(i^{th}\\) constraint. The \\(i^{th}\\) constraint is strictly met exactly when \\(w_i\\) is zero. Using the slack variables, the linear program (2.1) can be succinctly rewritten as: \\[\\begin{equation} w_1, \\dots, w_m, x_1, \\dots, x_n \\geq 0. \\end{equation}\\] Example 2.2 The slack variables for the linear program (1.2) are as follows: \\[\\begin{equation} \\begin{array}{rlllll} w_1 &amp; = &amp; 3.6 &amp; - &amp; 3x &amp; - &amp; 6y \\\\ w_2 &amp; = &amp; 1.5 &amp; - &amp; 2x &amp; - &amp; y \\\\ w_3 &amp; = &amp; 1 &amp; - &amp; x &amp; - &amp; y. \\end{array} \\end{equation}\\] In terms of these slack variables, the constraints can be rewritten as \\(x, y, w_1, w_2, w_3 \\ge 0\\) and the boundaries of the feasible region are given by \\(x = 0, y = 0, w_1 = 0, w_2 = 0, w_3 = 0\\). 2.2 Basic and non-basic variables Each vertex of the feasible region of a standard linear program in \\(n\\) variables is obtained by setting at least \\(n\\) variables (decision or slack) to zero. These variables are called non-basic and the remaining ones are called basic. Example 2.3 For the linear program (1.2), at the origin: the non-basic variables are \\(x, y\\) and the basic variables are \\(w_1, w_2, w_3\\), at the optimal solution: the non-basic variables are \\(w_1, w_2\\) and the basic variables are \\(x, y, w_3\\). Remark. Not every vertex obtained by setting \\(n\\) variables to zero is in the feasible region. For example, the vertex \\(x = 0, w_2 = 0\\) is not the in feasible region of the linear program (1.2). "],["simplex-method.html", "Chapter 3 Simplex method 3.1 Entering and leaving variables 3.2 Dictionaries 3.3 The simplex step 3.4 Stopping conditions", " Chapter 3 Simplex method The Simplex method is an iterative process for finding the optimal solution of a standard linear program. It starts at the some vertex of the feasible region and in each step moves to an adjacent vertex with a higher objective value. The following picture shows one possible run of the simplex algorithm for solving the linear program (1.2). Figure 3.1: A possible run of the simplex algorithm. 3.1 Entering and leaving variables In each step, one non-basic variable enters the set of basic variables and one basic variable leaves the set of basic variables. The table below explains how these sets are getting updated in the sample simplex algorithm run in Figure 3.1. Leaving variable Entering variable Basic variables Non-basic variables Start \\(\\{w_1, w_2, w_3\\}\\) \\(\\{x, y\\}\\) Step 1 \\(w_2\\) \\(x\\) \\(\\{w_1, x, w_3\\}\\) \\(\\{w_2, y\\}\\) Step 2 \\(w_1\\) \\(y\\) \\(\\{y, x, w_3\\}\\) \\(\\{w_2, w_1\\}\\) Our goal at each step is now reduced to figuring out the entering and leaving variables. 3.2 Dictionaries We’ll find the entering and leaving variables using dictionaries. A dictionary is a set of equations describing the objective function and the constraints in terms of the non-basic variables. Example 3.1 Consider (1.2) again. At the origin the non-basic variables are \\(x, y\\) and hence the initial dictionary is: \\[\\begin{equation} \\begin{array}{rlrrrr} \\mbox{objective} &amp; = &amp; 0 &amp; + &amp; 4x &amp; + &amp; 3y \\\\ w_1 &amp; = &amp; 3.6 &amp; - &amp; 3x &amp; - &amp; 6y \\\\ w_2 &amp; = &amp; 1.5 &amp; - &amp; 2x &amp; - &amp; y \\\\ w_3 &amp; = &amp; 1 &amp; - &amp; x &amp; - &amp; y. \\end{array} \\tag{3.1} \\end{equation}\\] After the first step of the simplex algorithm, the non-basic variables are \\(w_2, y\\). We can write \\(x\\) in terms of \\(w_2\\) to get \\[\\begin{equation} x = 0.75 - 0.5 w_2 - 0.5 y \\end{equation}\\] We can then substitute this into the initial dictionary to get the dictionary after the first step: \\[\\begin{equation} \\begin{array}{rlrrrr} \\mbox{objective} &amp; = &amp; 3 &amp; + &amp; (-2)w_2 &amp; + &amp; y \\\\ w_1 &amp; = &amp; 1.35 &amp; - &amp; (-1.5) w_2 &amp; - &amp; 4.5y \\\\ x &amp; = &amp; 0.75 &amp; - &amp; 0.5 w_2 &amp; - &amp; 0.5y \\\\ w_3 &amp; = &amp; 0.25 &amp; - &amp; (-0.5) w_2 &amp; - &amp; 0.5y. \\end{array} \\tag{3.2} \\end{equation}\\] Finally, the non-basic variables at the optimal solution are \\(w_1, w_2\\). We can repeat the above process and get the dictionary for the optimal solution: \\[\\begin{equation} \\begin{array}{rlrrrr} \\mbox{objective} &amp; = &amp; 3.3 &amp; + &amp; (-5/3)w_2 &amp; + &amp; (-2/9)w_1 \\\\ y &amp; = &amp; 0.3 &amp; - &amp; (-1/3) w_2 &amp; - &amp; 2/9 w_1 \\\\ x &amp; = &amp; 0.6 &amp; - &amp; 2/3 w_2 &amp; - &amp; (-1/9) w_1 \\\\ w_3 &amp; = &amp; 0.1 &amp; - &amp; (-1/3) w_2 &amp; - &amp; (-1/9) w_1. \\end{array} \\tag{3.3} \\end{equation}\\] Remark. From the dictionary, one can extract the set of basic variables and the set of non-basic variables by looking at the variables appearing on the LHS and RHS, respectively. Furthermore, by setting the non-basic variables to 0, we obtain the very useful fact that the values of the basic variables are simply the constants “\\(b_i\\)”, and the value of the objective function is the constant “\\(c_0\\)”. For example, from the final dictionary above, we can immediately see that \\(x = 0.6\\), \\(y = 0.3\\), \\(w_3 = 0.9\\), and the objective value is \\(3.3%\\) (and \\(w_1 = 0\\) and \\(w_2 = 0.3\\)) at the optimal solution. 3.3 The simplex step We’ll make several simplifying assumptions to begin with and then extend the algorithm to handle more complicated cases. To start, we’ll assume that at each vertex of the feasible region exactly \\(n\\) variables are non-basic. Such a linear program is called non-degenerate. Secondly, for the sake of explaining the algorithm, we’ll assume that out variable names and constants are dynamically updated i.e. after each step we rename the variables so that \\(\\{w_1, \\dots, w_m\\}\\) is the set of basic variables, \\(\\{x_1, \\dots, x_n\\}\\) is the set of non-basic variables and \\(c_j\\), \\(b_i\\), and \\(a_{ij}\\) are the constants appearing in the dictionary so that at the start of each step the dictionary is as follows: \\[\\begin{equation} \\begin{array}{rrrrrrrrrr} \\mbox{objective} &amp; = &amp; c_0 &amp; + &amp; c_1x_1 &amp; + &amp; \\dots &amp; + &amp; c_nx_n \\\\ w_1 &amp; = &amp; b_1 &amp; - &amp; a_{11} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{1n} x_n \\\\ w_2 &amp; = &amp; b_2 &amp; - &amp; a_{21} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{2n} x_n \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ w_m &amp; = &amp; b_m &amp; - &amp; a_{m1} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{mn} x_n \\end{array} \\tag{3.4} \\end{equation}\\] Note that you do not have to make this assumption when actually solving the linear program. This assumption is for exposition purposes only. 3.3.1 Choosing the entering variable We need to choose the entering variable to be one of the non-basic variables. This non-basic variable is going to increase from 0 to a positive value. Because the objective function has the following expression, \\[\\begin{align} \\mbox{objective} = c_0 + c_1x_1 + \\dots + c_nx_n \\end{align}\\] we can choose the variable \\(x_j\\) to be entering if and only if \\(c_j\\) is positive as increasing such a variable increases the objective value. We can think of the entering variable as determining the direction of the simplex step. Example 3.2 In the dictionary (3.1), the objective function is \\(4x + 3y\\). Hence, both \\(x\\) and \\(y\\) can be chosen as the entering variables. Geometrically, we can see that there are two different paths going from the origin to the optimal solution. In the dictionary (3.2), the objective function is \\(3 + (-2)w_2 + y\\). Hence, only \\(y\\) can be the entering variable. In dictionary (3.3), the objective function is \\(3.3 + (-5/3)w_2 + (-2/9)w_1\\). Hence, there cannot be any entering variable. 3.3.2 Choosing the leaving variable Suppose \\(x_j\\) is the chosen entering variable. Instead of choosing the leaving variable directly from among the basic variables, we find the largest value that the variable \\(x_j\\) can be increased to. We can think of finding the leaving variable as determining how far we can move in the direction of the simplex step without leaving the feasible region. As \\(x_j\\) increases \\(w_i\\) will decrease exactly when \\(a_{ij} &gt; 0\\). Because we want all the variables to be non-negative, we must always have \\(w_i = b_i - a_{ij} x_j \\ge 0\\). But this condition must hold true for all such \\(w_i\\). Hence, we get that \\(w_i\\) will be the leaving variable if \\[\\begin{align} i = {\\arg \\min} _{a_{ij} &gt; 0} \\dfrac{b_i}{a_{ij}} \\end{align}\\] In this case, after the simplex step \\(w_i \\to 0\\) and \\(x_j \\to {\\min} _{a_{ij} &gt; 0} \\dfrac{b_i}{a_{ij}}\\). Example 3.3 In the dictionary (3.1), if we choose \\(x\\) to be our entering variable then we need to get the following ratios to compare \\(i\\) \\(a_{ij}\\) \\(b_i\\) \\(b_i/a_{ij}\\) 1 3 3.6 1.2 2 2 1.5 0.75 3 1 1 1 We can see that the smallest ratio is obtained for \\(w_2\\) hence it is the only candidate for the leaving variable. 3.3.3 Tableau Once we have found the entering and leaving variables \\(x_j\\) and \\(w_i\\), we rewrite \\(x_j\\) in terms of \\(w_i\\) and the other non-basic variables to create the updated dictionary as in Example 3.1. This process can get extremely tedious to perform “by hand”. Instead, we introduce tableau to simplify the process. We start by rewriting the constraints in the dictionary with all the variables on the LHS and all the constants on the RHS: \\[\\begin{equation} \\begin{array}{rrrrrrrrrr} a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; + &amp; w_1 &amp; &amp; &amp; &amp; &amp; &amp; = &amp; b_1\\\\ a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; &amp; &amp; + &amp; w_2 &amp; &amp; &amp; &amp; = &amp; b_2\\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; &amp; &amp; &amp; &amp; &amp; + &amp; w_m &amp; = &amp; b_m\\\\ \\end{array} \\tag{3.5} \\end{equation}\\] This can then be encoded using the following augmented matrix: \\[\\begin{equation} \\begin{array}{rrrrrrrrrrr|r} a_{11} &amp; &amp; \\dots &amp; &amp; a_{1n} &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp;b_1\\\\ a_{21} &amp; &amp; \\dots &amp; &amp; a_{2n} &amp; &amp; &amp; 1 &amp; &amp; &amp; &amp;b_2\\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ a_{m1} &amp; &amp; \\dots &amp; &amp; a_{mn} &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp;b_m\\\\ \\end{array} \\end{equation}\\] We add back the objective function, but because of a quirk of algebra we need to add the objective function coefficients as follows. \\[\\begin{equation} \\begin{array}{rrrrrrrrrrr|r} c_1 &amp; &amp; \\dots &amp; &amp; c_{n} &amp; 0 &amp; &amp; 0 &amp; \\dots &amp; &amp; 0 &amp;-c_0\\\\ \\hline a_{11} &amp; &amp; \\dots &amp; &amp; a_{1n} &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp;b_1\\\\ a_{21} &amp; &amp; \\dots &amp; &amp; a_{2n} &amp; &amp; &amp; 1 &amp; &amp; &amp; &amp;b_2\\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ a_{m1} &amp; &amp; \\dots &amp; &amp; a_{mn} &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp;b_m\\\\ \\end{array} \\end{equation}\\] The columns in this augmented matrix correspond to the variables \\(w_i\\) and \\(x_j\\). The columns with the pivots correspond to the basic variables. If \\(x_j\\) is the entering variable and \\(w_i\\) is the leaving variable, then we simply perform elementary row operations and turn the entry \\(a_{ij}\\) into a pivot for its column. Hence, this step is also called the pivot step. Example 3.4 The tableau corresponding to the dictionary (3.1) is as follows: \\[\\begin{align} \\begin{bmatrix} 4 &amp; 3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 3 &amp; 6 &amp; 1 &amp; 0 &amp; 0 &amp; 3.6 \\\\ \\boxed{2} &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1.5 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\end{bmatrix} \\end{align}\\] If we choose \\(x\\) as the entering variable and \\(w_2\\) as the leaving variable then we need to pivot about the entry \\(a_{21}\\) using elementary row operations to get the following tableau: \\[\\begin{align} \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; -2 &amp; 0 &amp; -3 \\\\ 0 &amp; 4.5 &amp; 1 &amp; -1.5 &amp; 0 &amp; 1.35 \\\\ \\boxed{1} &amp; 0.5 &amp; 0 &amp; 0.5 &amp; 0 &amp; 0.75 \\\\ 0 &amp; 0.5 &amp; 0 &amp; -0.5 &amp; 1 &amp; 0.25 \\end{bmatrix} \\end{align}\\] which corresponds to the dictionary (3.2). 3.4 Stopping conditions Once we’ve found the entering and leaving variables, we can update the dictionary using the pivot step and start the process again. However, we might not always be able to find the entering and leaving variables. 3.4.1 No entering variable If no entering variable is found, then the geometry tells us that there is no direction in which the objective value can be increased i.e. we’re at a local maxima. But because the objective function is a linear function this local maxima is also an absolute maxima and provides an optimal solution to our linear program. Algebraically, this happens when none of the \\(c_i\\) are positive. 3.4.2 No leaving variable If no leaving variable is found, then the geometry tells us that we can keep increasing the entering variable indefinitely without leaving the feasible region. Such a linear program is called unbounded. An unbounded linear program has no optimal solution as the objective value can be made arbitrary large without leaving the feasible region. Algebraically, this happens when none of the \\(a_{ij}\\) are positive. "],["initialization.html", "Chapter 4 Initialization 4.1 Auxiliary linear program 4.2 Combined tableau", " Chapter 4 Initialization The simplex method starts at a vertex and tries to find an adjacent vertex with a higher objective value. We can start this process at the origin if it is a vertex of the feasible region. However, this is not always the case. The origin is in the feasible region of the standard linear program (2.1) if and only if \\(b_i \\ge 0\\) for all \\(1 \\le i \\le m.\\) When the origin is not a vertex of the feasible region, we need a process to find some vertex of the feasible region. This is called the initialization phase or Phase I of the simplex algorithm. 4.1 Auxiliary linear program We say that a linear program is feasible if its feasible region is non-empty. The initialization phase determines if a standard linear program is feasible and if it is then finds a vertex of the feasible region. To do this we create an auxiliary linear program whose optimal solution provides a feasible solution of the original linear program. The auxiliary linear program of the standard linear program (2.1) is defined as follows: \\[\\begin{equation} \\begin{array}{lrrrrrrrrrrr} \\mbox{maximize: } &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; - &amp; x_0 &amp; \\\\ \\mbox{subject to: } &amp; &amp; &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; - &amp;x_0 &amp; \\leq &amp; b_1 \\\\ &amp; &amp; &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; - &amp;x_0 &amp; \\leq &amp; b_2 \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ &amp; &amp; &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; - &amp;x_0 &amp; \\leq &amp; b_m \\\\ &amp; &amp; &amp; x_1, &amp; x_2, &amp; \\dots &amp;, &amp; x_n &amp; , &amp; x_0 &amp; \\geq &amp; 0 \\end{array} \\tag{4.1} \\end{equation}\\] One can show, using the extreme value theorem, that the auxiliary linear program (4.1) always has an optimal solution. Furthermore, using some basic algebraic manipulations one can prove the following theorem. Theorem 4.1 Suppose \\((k_1, k_2, \\dots, k_n, k_0)\\) is an optimal solution of the auxiliary linear program (4.1). Then, the standard linear program (2.1) is feasible if and only if \\(k_0 = 0\\). In this case, \\((k_1, k_2, \\dots, k_n)\\) is a vertex of the feasible region of (2.1). To understand the auxiliary linear program, it is better to write it in the following non-standard form: \\[\\begin{equation} \\begin{array}{lrrrrrrrrrrr} \\mbox{minimize: } &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; x_0 &amp; \\\\ \\mbox{subject to: } &amp; &amp; &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; \\leq &amp; b_1 &amp; + &amp; x_0 \\\\ &amp; &amp; &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; \\leq &amp; b_2 &amp; + &amp; x_0 \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ &amp; &amp; &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; \\leq &amp; b_m &amp; + &amp; x_0 \\\\ &amp; &amp; &amp; x_1, &amp; x_2, &amp; \\dots &amp;, &amp; x_n &amp; , &amp; x_0 &amp; \\geq &amp; 0 \\end{array} \\end{equation}\\] We can then interpret \\(x_0\\) as a relaxation of the constraints. The auxiliary linear program is then asking - what is the smallest constraint relaxation necessary to make our linear program feasible?. The primary linear program is feasible if and only if no relaxation is necessary. If the origin is not a vertex of the feasible region, then the method of solving the standard linear program (2.1), starting with Phase I, is as follows: Form the auxiliary linear program and its tableau. Note that we still cannot proceed normally as even for the auxiliary linear program the \\(b_i\\)’s are not all non-negative. Perform a pivot operation about the entry in the column corresponding to the variable \\(x_0\\) and the row corresponding to the most negative \\(b_i\\). This results in a dictionary where all the \\(b_i\\)’s are now non-negative. Solve the auxiliary linear program using the simplex method. Suppose \\((k_1, k_2, \\dots, k_n, k_0)\\) is an optimal solution of the auxiliary linear program. If \\(k_0 \\neq 0\\), then we halt as the primary linear program is not feasible. If \\(k_0 = 0\\), then we proceed to Phase II with the initial vertex \\((k_1, \\dots, k_n)\\). Find the dictionary at the initial vertex and proceed with the simplex method to find an optimal solution. 4.2 Combined tableau There is a shortcut to combining both Phase I and Phase II and reduce the number of pivots necessary. We create a combined tableau which contains information about both the auxiliary linear program and the primary linear program as follows: \\[\\begin{equation} \\begin{array}{rrrrrrrrrrrr|l} c_1 &amp; &amp; \\dots &amp; &amp; c_{n} &amp; 0 &amp; &amp; 0 &amp; \\dots &amp; &amp; 0 &amp; 0 &amp; 0\\\\ \\hline 0 &amp; &amp; \\dots &amp; &amp; 0 &amp; 0 &amp; &amp; 0 &amp; \\dots &amp; &amp; 0 &amp; -1 &amp; 0\\\\ \\hline a_{11} &amp; &amp; \\dots &amp; &amp; a_{1n} &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; -1 &amp;b_1\\\\ a_{21} &amp; &amp; \\dots &amp; &amp; a_{2n} &amp; &amp; &amp; 1 &amp; &amp; &amp; &amp; -1 &amp;b_2\\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\vdots \\\\ a_{m1} &amp; &amp; \\dots &amp; &amp; a_{mn} &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; -1 &amp;b_m\\\\ \\end{array} \\end{equation}\\] The first row of the tableau is the objective function of the primary linear program and the second row of the tableau is the objective of the auxiliary linear program. We use the combined tableau to first perform Phase I and then neglect the auxiliary objective and the variable \\(x_0\\) and proceed on to Phase II using the same tableau. Example 4.1 Consider the following linear program: \\[\\begin{equation} \\begin{array}{rrrrrl} \\mbox{maximize:} &amp; x &amp; + &amp; y \\\\ \\mbox{subject to:} &amp; x &amp; + &amp; 2y &amp; \\le &amp; 6 \\\\ &amp; -x &amp; &amp; &amp; \\le &amp; -1 \\\\ &amp; &amp; &amp; -y &amp; \\le &amp; -2 \\\\ &amp; x &amp; , &amp; y &amp; \\ge &amp; 0. \\end{array} \\end{equation}\\] It is easy to see that feasible region is given by the triangle with vertices \\((1,2)\\), \\((2,2)\\), and \\((1, 2.5)\\) and the optimal solution is given by \\((2,2)\\). Because \\((0,0)\\) is not feasible, we need the two-phase simplex method to solve this problem. We start with the combined tableau: \\[\\begin{align} \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 6 \\\\ -1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 &amp; 1 &amp; \\boxed{-1} &amp; -2 \\end{bmatrix} \\end{align}\\] The first pivot is in the column corresponding to the variable \\(x_0\\) and the row corresponding to the most negative \\(b_i\\) (which is the last row). This results in the following combined tableau: \\[\\begin{align} \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 2 \\\\ 1 &amp; 3 &amp; 1 &amp; 0 &amp; -1 &amp; 0 &amp; 8 \\\\ -1 &amp; 1 &amp; 0 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 2 \\end{bmatrix} \\end{align}\\] We then continue with the standard simplex method to find the solution: \\[\\begin{align} \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; -2 &amp; -3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; \\boxed{0} \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 2 &amp; -4 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 2 \\\\ 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 &amp; 1 \\end{bmatrix} \\end{align}\\] We see that the optimal value is 0 and hence the primary linear program is feasible. We then remove the auxiliary objective and coefficient to get the following tableau for the primary linear program: \\[\\begin{align} \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; -3 \\\\ 0 &amp; 0 &amp; 1 &amp; \\boxed{1} &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 2 \\\\ 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 \\end{bmatrix} \\end{align}\\] We continue solving this using the simplex method to get the final tableau \\[\\begin{align} \\begin{bmatrix} 0 &amp; 0 &amp; -1 &amp; 0 &amp; -1 &amp; -4 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 2 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 2 &amp; 2 \\end{bmatrix} \\end{align}\\] The final non-basic variables are \\(w_1\\) and \\(w_3\\) and the basic variables are \\(x, y, w_2\\) with values \\(2, 2, 1\\), respectively. The optimal objective value is \\(4\\). "],["halting-problem.html", "Chapter 5 Halting problem 5.1 Fundamental theorem of linear programming 5.2 Degeneracy 5.3 Bland’s rule", " Chapter 5 Halting problem We now have a complete algorithm for solving a standard linear program. However, we have not shown that it always finds the optimal solution. Let us start with the existence of optimal solution. 5.1 Fundamental theorem of linear programming To resolve the issue of getting stuck in a loop, we first need the following theorem about linear programs: Theorem 5.1 (Fundamental theorem of linear programming) Every standard linear program is exactly one of the following: Infeasible, Unbounded, Has an optimal solution. In this case, the optimal solution is obtained at one of the vertices of the feasible region. This theorem is an easy corollary of the extreme value theorem for multi-variate functions and relies on the closedness of the feasible region of a standard linear program and the continuity of the objective function. We see that the simplex method is able to detect all three cases. The first case happens when the Phase I algorithm is unable to find a feasible solution, the second case happens when no leaving variable is found, and the third case happens when no entering variable is found. However, it is possible for the simplex method to get stuck in a loop. This is called cycling. One way to show that the simplex method does not cycle is to show that the objective value always increases after the simplex step. If this were the case then we would never visit the same vertex twice. The vertices of the feasible region are obtained by intersecting constraint boundaries and hence only are finite in number (even if the region is unbounded). So, if we never visit the same vertex twice, the simplex method must halt. However, it is not always the case that the objective value increases after a simplex step! 5.2 Degeneracy We saw in Section 3.3.2 that after the simplex step the entering and leaving variable gets updated as follows: \\[\\begin{align} x_j &amp; \\mapsto b_i/a_{ij} \\\\ w_i &amp; \\mapsto 0. \\end{align}\\] This increases the value of the objective function by \\(c_j b_i/a_{ij}\\). Because of the criterion for choosing the entering and leaving variables, the constants \\(c_i\\) and \\(a_{ij}\\) are always positive. We know that \\(b_i\\) this is the value of the basic variable \\(w_i\\) and hence must be \\(\\ge 0\\). But we cannot guarantee that \\(b_i &gt; 0\\). If \\(b_i = 0\\), then the objective value does not increase after the simplex step. This can result in the simplex method to get stuck in a loop. We say that the dictionary at a feasible vertex is degenerate if some \\(b_i = 0\\). In this case, one of the basic variables also has the value 0 at this vertex. Geometrically, this is saying that more than \\(n\\) constraints are satisfied at this vertex. Example 5.1 The following slight modification of Example (1.2) is a degenerate linear program: \\[\\begin{equation} \\begin{array}{rrrrrl} \\mbox{maximize:} &amp; 4x &amp; + &amp; 3y \\\\ \\mbox{subject to:} &amp; 3x &amp; + &amp; 6y &amp; \\le &amp; 4.5 \\\\ &amp; 2x &amp; + &amp; y &amp; \\le &amp; 1.5 \\\\ &amp; x &amp; + &amp; y &amp; \\le &amp; 1 \\\\ &amp; x &amp; , &amp; y &amp; \\ge &amp; 0. \\end{array} \\end{equation}\\] At the optimal solution, \\((0.5, 0.5)\\) all three constraints are met. At this vertex, one of the rows is \\[\\begin{align} w_3 &amp;= 0 + 0.33 w_2 + 0.11 w_1. \\end{align}\\] Example 5.2 Consider the following degenerate linear program: \\[\\begin{equation} \\begin{array}{rrrrrrrl} \\mbox{maximize:} &amp; x_1 &amp; - &amp; 2x_2 &amp; &amp; &amp; - &amp; 2x_4 \\\\ \\mbox{subject to:} &amp; 0.5 x_1 &amp; - &amp; 3.5x_2 &amp; - &amp; 2x_3 &amp; + &amp; 4 x_4 &amp; \\le &amp; 0 \\\\ &amp; 0.5 x_1 &amp; - &amp; x_2 &amp; - &amp; 0.5 x_3 &amp; + &amp; 0.5 x_4 &amp; \\le &amp; 0 \\\\ &amp; x_1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\le &amp; 1 \\\\ &amp; x_1 &amp; , &amp; x_2 &amp; , &amp; x_3 &amp; , &amp; x_4 &amp; \\ge &amp; 0 \\\\ \\end{array} \\end{equation}\\] The following is a valid sequence of simplex steps: \\(x_1\\) enters and \\(w_1\\) leaves, \\(x_2\\) enters and \\(w_2\\) leaves, \\(x_3\\) enters and \\(x_1\\) leaves, \\(x_4\\) enters and \\(x_2\\) leaves, \\(w_1\\) enters and \\(x_3\\) leaves, \\(w_2\\) enters and \\(x_4\\) leaves. At the end of the \\(6^{th}\\) simplex step, we end up looping back to the origin. 5.3 Bland’s rule There are various ways of dealing with cycling. The simplex way is to remember all the simplex steps taken so far and if we happen to revisit a vertex then we simple make a different choice of entering and leaving variables. However, as it turns out, there is a much simpler way to avoid cycling by using Bland’s rule. Bland’s rule says that if there are multiple candidates for the entering/variable then we choose the one with the smallest index. (We assume that the decision variables have a smaller index than the slack variables.) Theorem 5.2 (Bland's rule) The simplex method always terminates provided that both the entering and the leaving variable are chosen according to Bland’s rule. The proof of this theorem is too complicated for this course. With this modification, for both Phase I and Phase II of the simplex method, we now have a complete algorithm for solving linear programs. Example 5.3 In Example 5.2, the sixth simplex step violates Bland’s rule. Once we choose the entering and leaving variables using Bland’s rule, we can check that the simplex step for Example 5.2 indeed terminates at the optimal solution. "],["standardization.html", "Chapter 6 Standardization", " Chapter 6 Standardization A general linear program is an optimization question where the objective function is a linear program and the constraints are linear equalities or inequalities. Remark. We do not allow the inequalities to be “&lt;” or “&gt;”. This is because the region defined by using such inequalities is an open set and extreme value theorem is not applicable to these open sets. This further implies that the fundamental theorem of linear programming is no longer valid for these kinds of constraints. For example, consider the following linear program: \\[\\begin{align} \\mbox{maximize: } &amp; x \\\\ \\mbox{subject to: } &amp; x &lt; 1 \\\\ &amp; x \\geq 1. \\\\ \\end{align}\\] The feasible region is \\([0, 1)\\) which is bounded from above and yet there is no optimal solution. Given a general linear program, we can standardize it by a sequence of algebraic transformations and convert it to a standard linear program of the form (2.1). 6.0.1 Objective If the goal is to minimize the objective function then to convert it to a maximization problem, we simply multiply the objective function by \\(-1\\). Minimizing a function \\(f\\) is the same as maximizing \\(-f\\). 6.0.2 Constraints We first move all the variables in the constraints to the LHS and the constants to the RHS and then fix the constraints one at a time. If a constraint has the inequality \\(\\geq\\) then we multiply both sides of the constraint by -1 to change the inequality to \\(\\leq\\). If a constraint is an equality constraint, then we replace the equality with two constraints: one with the inequality \\(\\leq\\) and one with the inequality \\(\\geq\\). 6.0.3 Variables If a variable has a non-positivity constraint of the form \\(x_j \\le 0\\), then we replace \\(x_j\\) with \\(-x_j\\) everywhere. If a variable \\(x_j\\) is free i.e. it does not have any sign constraints on it, then we introduce two new variables \\(x_j&#39;\\), \\(x_j&#39;&#39;\\) which are both non-negative and make the substitution \\(x_j = x_j&#39; - x_j&#39;&#39;\\). This trick works because any real number can be written as a difference of two non-negative reals. "],["dual-linear-program.html", "Chapter 7 Dual linear program", " Chapter 7 Dual linear program In the next few chapters we’ll prove duality theorems about linear programs. We start by introducing the matrix notation. We’ll let \\(x\\) denote the vector of decision variables, \\(b\\) the vector of upper bounds, \\(c\\) the vector of objective coefficients, and \\(A\\) the matrix of constraints in the standard linear program (2.1). \\[\\begin{align} x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}, \\quad c = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}, \\quad A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\dots &amp; a_{mn} \\end{bmatrix}. \\end{align}\\] We’ll assume that \\(c_0 = 0\\) at the start. The standard linear program (2.1) can be written as follows: \\[\\begin{equation} \\begin{array}{lrll} \\mbox{maximize: } &amp; c^T x \\\\ \\mbox{subject to: } &amp; A x &amp; \\leq &amp; b \\\\ &amp; x &amp; \\geq &amp; 0. \\end{array} \\end{equation}\\] The dual of this linear program is defined as the following linear program: \\[\\begin{equation} \\begin{array}{lrll} \\mbox{minimize: } &amp; b^T y \\\\ \\mbox{subject to: } &amp; A^T y &amp; \\geq &amp; c \\\\ &amp; y &amp; \\geq &amp; 0, \\end{array} \\tag{7.1} \\end{equation}\\] where \\(y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\\) is the vector of dual decision variables. The original linear program is called the primal. The dual decision variables correspond to the constraints of the original linear program. By explicitly computation, we can easily prove that the following theorem. Theorem 7.1 The (standardization of) the dual of the (standardized) dual is the primal. Hence, we think of linear programs as coming in primal-dual pairs. Every linear program has a dual and it is itself the dual of the dual. The dual decision variables correspond to the constraints of the original linear program and the primal decision variables correspond to the constraints of the dual. Example 7.1 The dual of (1.2) is: \\[\\begin{equation} \\begin{array}{lrrrrrrrrrr} \\mbox{minimize: } &amp; 3.6 y_1 &amp; + &amp; 1.5 y_2 &amp; + &amp; y_3 \\\\ \\mbox{subject to: } &amp; 3y_1 &amp; + &amp; 2 y_2 &amp; + &amp; y_3 &amp; \\geq &amp; 4 \\\\ &amp; 6y_1 &amp; + &amp; y_2 &amp; + &amp; y_3 &amp; \\geq &amp; 3 \\\\ &amp; y_1 &amp; , &amp; y_2 &amp; , &amp; y_3 &amp; \\geq &amp; 0. \\end{array} \\tag{7.2} \\end{equation}\\] The variable \\(y_1\\) corresponds to the “maturity”, the variable \\(y_2\\) corresponds to the “risk”, and the variable \\(y_3\\) corresponds to the “percentage”. We can think of these variables as “internal costs/prices”. The expression \\(3y_1 + 2 y_2 + y_3\\) is the “price” of the corporate bond and the expression \\(6 y_1 + y_2 + y_3\\) is the “price” of the government bond. The dual constraints are saying that the “internal price” of either of the two bonds should be at least as large as the “external price” i.e. yield of the corporate bond. "],["weak-and-strong-duality.html", "Chapter 8 Weak and strong duality 8.1 Weak duality 8.2 Strong duality", " Chapter 8 Weak and strong duality Consider the standard linear program (primal) (2.1) and its dual (7.1). We say that a vector \\(x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\) is primal feasible if it is in the feasible region of the primal and a vector \\(y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\\) is dual feasible if it is in the feasible region of the dual. 8.1 Weak duality Theorem 8.1 (Weak duality) Suppose \\(x\\) is primal feasible and \\(y\\) is dual feasible. Then the primal objective value at \\(x\\) is less than or equal to the dual objective value at \\(y\\). Proof. The proof relies on analyzing the term \\(y^T A x\\) and follows by looking at the following sequence of inequalities: \\[\\begin{align} b^T y &amp; = y^T b \\\\ &amp; \\ge y^T (Ax) &amp;&amp; \\mbox{ as } Ax \\leq b \\mbox{ and } y \\geq 0 \\\\ &amp; = (y^T A x)^T \\\\ &amp; = x^T A^T y \\\\ &amp; \\ge x^T c &amp;&amp; \\mbox{ as } A^Ty \\geq c \\mbox{ and } x \\geq 0 \\\\ &amp; = c^T x. \\end{align}\\] We get several immediate corollaries out of Weak duality. Corollary 8.1 If the primal is unbounded, then the dual is infeasible. Corollary 8.2 If the dual is unbounded, then the primal is infeasible. Corollary 8.3 If both the primal and dual have optimal solutions, then the optimal value of the primal is less than or equal to the optimal value of the dual. We cannot say anything about the dual in the case when the primal is infeasible. Similarly, we cannot conclude anything about the existence of an optimal value of the dual in the case when the primal has an optimal solution. Some variant of weak duality usually exists for optimization problems which are non-linear. However, the strong duality theorem stated below is more uncommon and only holds true under very special conditions on the constraints. 8.2 Strong duality The tableau of the primal problem (2.1) is as follows: \\[\\begin{equation} \\begin{array}{ll|r} c^T &amp; 0 &amp; c_0 \\\\ \\hline A &amp; I_m &amp; b \\end{array} \\tag{8.1} \\end{equation}\\] We can standardize the dual problem (7.1) and form its tableau: \\[\\begin{equation} \\begin{array}{ll|r} -b^T &amp; 0 &amp; -c_0 \\\\ \\hline -A^T &amp; I_n &amp; -c \\end{array} \\tag{8.2} \\end{equation}\\] We will such tableaus duals of each other. More generally, we’ll say that two tableaus (of appropriate dimensions) are duals of each other if after rearranging the pivot columns, if necessary, they’re of the above form. One can show the following theorem by explicit computation: Lemma 8.1 Consider the two dual tableaus (8.1) and (8.2) . If we pivot the first tableau about the \\(i^{th}\\) row and \\(j^{th}\\) column of \\(A\\) and the second tableau about the \\(i^{th}\\) column and \\(j^{th}\\) row of \\(-A^T\\), then the resulting tableaus remain duals of each other. Lemma 8.2 If the tableau (8.1) corresponds to an optimal solution of the primal then the tableau (8.2) corresponds to an optimal solution of the dual. Proof. The tableau (8.1) corresponds to an optimal solution of the primal precisely when (primal optimality) \\(c^T \\le 0\\) as in this case no entering variable can be found for the primal, and (primal feasibility) \\(b \\ge 0\\). These conditions translate to (dual optimality) \\(-b^T \\le 0\\) as in this case no entering variable can be found for the dual, and (dual feasibility) \\(-c \\ge 0\\). Using the above two lemmas, and by explicitly running the simplex method we get the following result: Theorem 8.2 (Strong duality) If the primal has an optimal solution then so does the dual. Moreover, they have the same optimal values. Proof. At the optimal solution for the primal, we have a set of basic and non-basic variables. We can perform a sequence of pivot operations to get this tableau from the initial tableau. We then perform the corresponding pivots on the dual tableau. By Lemma 8.1 the resulting tableau will be dual to the primal tableau at the optimal solution. By Lemma 8.2 the dual is also optimal and has the same objective value. "],["certificate-of-optimality.html", "Chapter 9 Certificate of optimality 9.1 Complimentary slackness", " Chapter 9 Certificate of optimality We can use the two duality theorems to come up with a fast way to check optimality. Theorem 9.1 (Certificate of optimality) \\(x\\) is an optimal solution for the primal and \\(y\\) is an optimal solution for the dual if and only if \\(x\\) is primal-feasible, \\(y\\) is dual-feasible, \\(c^T x = b^T y\\) i.e. the primal objective value at \\(x\\) is equal to the dual objective value at \\(y\\). Proof. ( \\(\\Rightarrow\\) ) If \\(x\\) and \\(y\\) are optimal solutions, then they are feasible by definition and by strong duality (Theorem 8.2) they have the same objective value. ( \\(\\Leftarrow\\) ) If \\(x\\) and \\(y\\) are feasible solutions then by weak duality (Theorem 8.1) the dual objective values provide an upper bound on the primal objective value. Because this upper bound is attained at \\(x\\), \\(x\\) must be an optimal solution of the primal. Similarly, for \\(y\\). 9.1 Complimentary slackness There is another closely related method for verifying the correctness of solution using primal and dual slack variables. Denote by \\(w = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{bmatrix}\\) the primal slack variables and by \\(z = \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\) the dual slack variables. More explicitly, \\[\\begin{align} w &amp;= b - A x \\\\ z &amp;= -c + A^T y. \\end{align}\\] We use this convention for \\(z\\) as then at a dual feasible solution \\(z \\ge 0\\). Theorem 9.2 (Complementary slackness) Suppose \\(x\\) is primal feasible and \\(y\\) is dual feasible. Then \\(x\\) and \\(y\\) are optimal if and only if for all \\(1 \\le j \\le n\\), \\(x_j z_j = 0\\), and for all \\(1 \\le i \\le m\\), \\(y_i w_i = 0\\). Proof. The proof is in two steps. We first show a weaker statement about the vanishing of two scalars and then show that the vanishing of these scalars implies complementary slackness. Claim: \\(x\\) and \\(y\\) are optimal solutions if and only if \\(x^T z = 0\\) and \\(y^T w = 0\\). We start by rewriting the slack variable: \\[\\begin{align} &amp;&amp; x^T z = 0 &amp;&amp; \\mbox{ and } &amp;&amp; y^T w = 0 \\\\ \\Leftrightarrow &amp;&amp; x^T (-c + A^T y) = 0 &amp;&amp; \\mbox{ and } &amp;&amp; y^T (b - A x) = 0 \\\\ \\Leftrightarrow &amp;&amp; x^T c = x^T A^T y &amp;&amp; \\mbox{ and } &amp;&amp; y^T b = y^T A x \\\\ \\Leftrightarrow &amp;&amp; c^T x = y^T A x &amp;&amp; \\mbox{ and } &amp;&amp; b^T y = y^T A x \\\\ \\end{align}\\] Thus we are reduced to showing that \\(x\\) and \\(y\\) are optimal solutions if and only if \\(c^T x = y^T A x\\) and \\(b^T y = y^T A x\\). ( \\(\\Leftarrow\\) ) As \\(c^T x = y^T A x = b^T y\\), and \\(x\\) and \\(y\\) are given to be feasible, \\(x\\) and \\(y\\) are optimal by Theorem 9.1. ( \\(\\Rightarrow\\) ) By the proof of weak duality 8.1, we know that as \\(x\\) and \\(y\\) are feasible, \\[c^T x \\le y^T A x \\le b^T y.\\] By strong duality, as \\(x\\) and \\(y\\) are optimal \\[c^T x = b^T y.\\] The only way the two can be simultaneously true is if \\(c^T x = y^T A x\\) and \\(b^T y = y^T A x\\). Claim: \\(x^T z = 0\\) and \\(y^T w = 0\\) if and only if for all \\(1 \\le j \\le n\\), \\(x_j z_j = 0\\), and for all \\(1 \\le i \\le m\\), \\(y_i w_i = 0\\). This follows from the fact that at a feasible solution \\(x, y, w, z \\ge 0\\). "],["dual-of-a-general-linear-program.html", "Chapter 10 Dual of a general linear program", " Chapter 10 Dual of a general linear program We saw how to standardized non-standard linear programs in Chapter 6. We need to “fix” the objective function, constraints, and signs of decision variables. To find the dual of a non-standard linear program, we can first standardize it and then form the dual. However, doing so changes the coefficients of the constraints. After standardization, we can “revert the standardization” of the dual so that the matrix that defines the dual constraints is precisely the transpose of the matrix that defines the primal constraints. This requires us to change appropriate inequalities in the dual as described in the following table. Primal Dual \\(a_{i1} x_1 + \\dots + a_{in} x_n \\le b_i\\) \\(y_i \\ge 0\\) \\(a_{i1} x_1 + \\dots + a_{in} x_n \\ge b_i\\) \\(y_i \\le 0\\) \\(a_{i1} x_1 + \\dots + a_{in} x_n = b_i\\) \\(y_i\\) is free \\(x_j \\ge 0\\) \\(a_{1j}y_1 + \\dots + a_{mj} y_m \\ge c_j\\) \\(x_j \\le 0\\) \\(a_{1j}y_1 + \\dots + a_{mj} y_m \\le c_j\\) \\(x_j\\) is free \\(a_{1j}y_1 + \\dots + a_{mj} y_m = c_j\\) Note that if we define slack variables as before \\[\\begin{align} w &amp;= b - A x \\\\ z &amp;= -c + A^T y, \\end{align}\\] then even for a general linear program we will have \\(x_j w_j \\ge 0\\) for all \\(1 \\le j \\le n\\) and \\(y_i z_i \\ge 0\\) for all \\(1 \\le i \\le m\\). Hence, complementary slackness, as stated in Theorem 9.2, holds true for the general linear programs. "],["sensitivity-analysis---constraints.html", "Chapter 11 Sensitivity analysis - Constraints 11.1 Matrix notation 11.2 Range of optimality 11.3 Rate of change 11.4 Dual variables", " Chapter 11 Sensitivity analysis - Constraints Linear programs are used to model real world problems. Such models are at best approximate and at worst inaccurate. As such, it is important to understand the sensitivity of our solution to changes in the model. This is broadly called sensitivity analysis. We will focus on understanding the dependence of the optimal objective value of the standard linear program (2.1) on the constants \\(b_i\\) and \\(c_j\\). Throughout this chapter, we’ll assume that our linear programs have an optimal solution. 11.1 Matrix notation In our previous analysis of the simplex method, we assumed that the constants \\(b_i\\), \\(c_j\\), and \\(a_{ij}\\) were being dynamically updated. However, now we’ll assume that they are fixed constants as our goal is no longer to run the simplex algorithm but rather to find the dependence of the solution on the initial values of these constants. We will start by finding a succinct way to describe the dictionary at the optimal solution. Recall that the decision and slack variables are related to each other by the Equation (3.5) which can be written as: \\[\\begin{equation} \\begin{bmatrix} A &amp; I_m \\end{bmatrix} \\begin{bmatrix} x \\\\ w \\end{bmatrix} = b. \\tag{11.1} \\end{equation}\\] Let \\(\\widehat{A} := \\begin{bmatrix} A &amp; I_m \\end{bmatrix}\\). We’ll decompose \\(\\widehat{A}\\) using the basic and non-basic variables. Then let \\(\\mathcal{B}\\) be the matrix formed by combining the columns of \\(\\widehat{A}\\) corresponding to the basic variables and let \\(\\mathcal{N}\\) be the matrix formed by combining the columns of \\(\\widehat{A}\\) corresponding to the non-basic variables. Let \\(x_{\\mathcal{B}}\\) be the vector of basic variables and \\(x_{\\mathcal{N}}\\) be the vector of non-basic variables. By rearranging the columns of \\(\\widehat{A}\\) if necessary, we can rewrite (11.1) as \\[\\begin{align} &amp;&amp; \\begin{bmatrix} \\mathcal{B} &amp; \\mathcal{N} \\end{bmatrix} \\begin{bmatrix} x_{\\mathcal{B}} \\\\ x_{\\mathcal{N}} \\end{bmatrix} &amp;= b,\\\\ \\implies &amp;&amp; \\mathcal{B} x_{\\mathcal{B}} + \\mathcal{N} x_{\\mathcal{N}} &amp;= b, \\\\ \\implies &amp;&amp; \\mathcal{B} x_{\\mathcal{B}} &amp;= b - \\mathcal{N} x_{\\mathcal{N}}. \\end{align}\\] One can show that the matrix \\(\\mathcal{B}\\) is always invertible (hence the name “basic” variables) and the above equation can be further simplified to the following: \\[\\begin{equation} x_{\\mathcal{B}} = \\mathcal{B}^{-1} b - \\mathcal{B}^{-1} \\mathcal{N} x_{\\mathcal{N}}. \\tag{11.2} \\end{equation}\\] This is nothing but the dictionary at the optimal solution. Example 11.1 Consider Example (1.2) again. At the optimal solution \\(w_1\\) and \\(w_2\\) are non-basic and have the value 0, and \\(x\\), \\(y\\), and \\(w_3\\) are basic with values \\(0.3\\), \\(0.6\\), and \\(0.9\\), respectively. Using the above notation, we have \\[\\begin{align} \\mathcal{B} = \\begin{bmatrix} 3 &amp; 6 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}, x_{\\mathcal{B}} = \\begin{bmatrix} x \\\\ y \\\\ w_3 \\end{bmatrix}, \\\\ \\mathcal{N} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}, x_{\\mathcal{N}} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}. \\end{align}\\] Using Equation (11.2) the dictionary at the optimal solution becomes \\[\\begin{align} \\begin{bmatrix} x \\\\ y \\\\ w_3 \\end{bmatrix} &amp;= \\begin{bmatrix} 3 &amp; 6 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} 3.6 \\\\ 1.5 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 3 &amp; 6 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} -1/9 &amp; 2/3 &amp; 0 \\\\ 2/9 &amp; -1/3 &amp; 0 \\\\ -1/9 &amp; -1/3 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 3.6 \\\\ 1.5 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} -1/9 &amp; 2/3 &amp; 0 \\\\ 2/9 &amp; -1/3 &amp; 0 \\\\ -1/9 &amp; -1/3 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} - \\begin{bmatrix} -1/9 &amp; 2/3 \\\\ 2/9 &amp; -1/3 \\\\ -1/9 &amp; -1/3 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}. \\end{align}\\] This is precisely the dictionary (3.3) at the optimal solution. Because we set the non-basic variables \\(x_{\\mathcal{N}}\\) to 0 at any vertex, and in particular, at the the optimal solution, using the Equation (11.2) we get the following useful result: Lemma 11.1 Using the above notation, \\(\\mathcal{B}^{-1}b\\) is the value of the basic variables \\(x^*_{\\mathcal{B}}\\) at the optimal solution. 11.2 Range of optimality We now try to determine the change in optimal solution as we change the constraint upper bounds \\(b_i\\). It is likely that by changing \\(b_i\\) we change the optimal solution. But we would want this change to be differentiable. This can be achieved by requiring the set of basic and non-basic variables to remain unchanged. In this case, the equation (11.2) will still be the equation describing the dictionary at the optimal solution and the change in \\(b_i\\) will result in a differentiable (in fact, linear) change in \\(x_{\\mathcal{B}}\\). Example 11.2 Suppose we vary \\(b_3 = 1\\) in Example (1.2). One can check that at the optimal solution \\(w_1\\) and \\(w_2\\) are non-basic as long as \\(b_3 &gt; 0.9\\). Thus we can say that out model is a good model as long as the error in \\(b_3\\) is less than \\(0.1\\). Suppose we change \\(b_i\\) to \\(b_i + \\delta\\), where \\(\\delta\\) is a real number, and leave all the other constants unchanged. This is equivalent to changing \\(b\\) to \\(b + \\delta e_i\\) where \\(e_i\\) is the \\(i^{th}\\) standard basis vector. This changes equation (11.2) to \\[\\begin{align} x_{\\mathcal{B}} &amp;= \\mathcal{B}^{-1} b + \\delta \\mathcal{B}^{-1} e_i - \\mathcal{B}^{-1} \\mathcal{N} x_{\\mathcal{N}} \\\\ &amp;= \\mathcal{B}^{-1} b + \\delta (\\mathcal{B}^{-1})_{\\_i} - \\mathcal{B}^{-1} \\mathcal{N} x_{\\mathcal{N}}. \\end{align}\\] where \\((\\mathcal{B}^{-1})_{\\_i}\\) denotes the \\(i^{th}\\) column of \\(\\mathcal{B}^{-1}\\). Note that the coefficients of \\(x_{\\mathcal{N}}\\) remain unchanged. So, for this dictionary to stay optimal we only need the constants to remain non-negative i.e. \\[\\begin{equation} \\mathcal{B}^{-1} b + \\delta (\\mathcal{B}^{-1})_{\\_i} \\ge 0. \\tag{11.3} \\end{equation}\\] The range of optimality for \\(b_i\\) is the interval \\((b_i + \\delta_-, b_i + \\delta_+)\\) such that \\(\\mathcal{B}^{-1} b + \\delta (\\mathcal{B}^{-1})_{\\_i} \\ge 0\\) for all \\(\\delta \\in (\\delta_-, \\delta_+)\\). In practice, Equation (11.3) gives us \\(m\\) inequalities, all of which need to be simultaneously satisfied. These give us candidate values for \\(\\delta\\) some of which are positive and some of which are negative. We then choose \\(\\delta_+\\) to be the smallest positive value and \\(\\delta_-\\) to be the largest negative value. If \\(\\delta_+\\) does not exist the upper bound is \\(\\infty\\) and if \\(\\delta_-\\) does not exist the lower bound is \\(-\\infty\\). If either \\(\\delta_+\\) or \\(\\delta_-\\) is 0 then the linear program is degenerate and the range of optimality for \\(b_i\\) is empty. In this case, our program is very sensitive to perturbations in \\(b_i\\). Example 11.3 Let us find the range of optimality for \\(b_1 = 3.6\\), \\(b_2=1.5\\), and \\(b_3 = 1\\) in (1.2) using our calculations in Example 11.1. We know that \\[\\begin{align} \\mathcal{B}^{-1} = \\begin{bmatrix} -1/9 &amp; 2/3 &amp; 0 \\\\ 2/9 &amp; -1/3 &amp; 0 \\\\ -1/9 &amp; -1/3 &amp; 1 \\end{bmatrix}. \\end{align}\\] Using \\(i = 1\\) and Lemma 11.1 in Equation (11.3) we get \\[\\begin{align} \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} + \\delta \\begin{bmatrix} -1/9 \\\\ 2/9 \\\\ -1/9 \\end{bmatrix} &gt; 0 \\end{align}\\] which gives us the inequalities \\[\\begin{align} \\begin{array}{lrlrrll} 0.6 + \\delta (-1/9) &amp;\\ge &amp; 0 &amp; \\implies &amp; \\delta &amp;\\le &amp; 0.6 (9) = 5.4 \\\\ 0.3 + \\delta (2/9) &amp;\\ge &amp; 0 &amp; \\implies &amp; \\delta &amp;\\ge &amp; -0.3 (9/2) = -1.35 \\\\ 0.1 + \\delta (-1/9) &amp;\\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\le &amp; 0.1 (9) = 0.9. \\end{array} \\end{align}\\] So, \\(\\delta_- = -1.35\\) and \\(\\delta_+ = \\min(5.4, 0.9) = 0.9\\) and the range of optimality for \\(b_1\\) is \\((3.6 - 1.35, 3.6 + 0.9) = (2.85, 3.45)\\). Using \\(i = 2\\) and Lemma 11.1 in Equation (11.3) we get \\[\\begin{align} \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} + \\delta \\begin{bmatrix} 2/3 \\\\ -1/3 \\\\ -1/3 \\end{bmatrix} &gt; 0 \\end{align}\\] which gives us the inequalities \\[\\begin{align} \\begin{array}{lrlrrll} 0.6 + \\delta (2/3) &amp;\\ge&amp; 0 &amp; \\implies &amp; \\delta &amp;\\ge&amp; - 0.6 (3/2) = -0.9 \\\\ 0.3 + \\delta (-1/3) &amp;\\ge &amp;0 &amp; \\implies &amp; \\delta &amp;\\le &amp;0.3 (3) = 0.9 \\\\ 0.1 + \\delta (-1/3) &amp;\\ge &amp;0 &amp; \\implies &amp; \\delta &amp; \\le&amp; 0.3 (1) = 0.3. \\end{array} \\end{align}\\] So, \\(\\delta_- = -0.9\\) and \\(\\delta_+ = \\min(0.3, 0.9) = 0.3\\) and the range of optimality for \\(b_2\\) is \\((1.5 - 0.9, 1.5 + 0.3) = (0.6, 1.8)\\). Using \\(i = 3\\) and Lemma 11.1 in Equation (11.3) we get \\[\\begin{align} \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} + \\delta \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} &gt; 0 \\end{align}\\] which gives us \\(\\delta \\ge -0.1\\) and so the range of optimality for \\(b_3\\) is \\((1 - 0.1, \\infty) = (0.9, \\infty)\\). 11.3 Rate of change Assume now that neither of \\(\\delta_+\\) or \\(\\delta_-\\) is zero. Then, we can use Lemma 11.1 to find the rate of change of the optimal solution with respect to \\(b_i\\). Call the objective function \\(\\mathbb{O} = c^{T} x^*\\). We think of \\(\\mathbb{O}\\) as being a function of \\(b_i\\), \\(c_j\\), and \\(a_{ij}.\\) Using Lemma 11.1 we get \\[\\begin{align} \\dfrac{\\partial x^*_{\\mathcal{B}_j}}{\\partial b_i} &amp;= j^{th} \\mbox{ row of } \\dfrac{\\partial \\mathcal{B}^{-1}b}{\\partial b_i} \\\\ &amp;= (\\mathcal{B}^{-1})_{ji} \\\\ \\end{align}\\] where \\(x^*_{\\mathcal{B}_j}\\) denotes the \\(j^{th}\\) basic variable at the optimal solution and \\[\\begin{align} \\dfrac{\\partial x^*_{\\mathcal{N}}}{\\partial b_i} &amp;= 0 \\end{align}\\] as the non-basic variables remain 0 when we perturb \\(b_i\\) within the range of optimality. Using these we can find the rate of change of the optimal solution \\(\\mathbb{O}\\) with respect to \\(b_i\\). We first start by re-indexing the variables and objective coefficients using the basic and non-basic variables. \\[\\begin{align} \\mathbb{O} &amp;= c^T x^* \\\\ &amp;= c^T_{\\mathcal{B}} x^*_{\\mathcal{B}} + c^T_{\\mathcal{N}} x^*_{\\mathcal{N}} \\\\ \\implies \\dfrac{\\partial \\mathbb{O}}{\\partial b_i} &amp;= c^T_{\\mathcal{B}} \\dfrac{\\partial x^*_{\\mathcal{B}}}{\\partial b_i} + c^T_{\\mathcal{N}} \\dfrac{\\partial x^*_{\\mathcal{N}}}{\\partial b_i} \\\\ &amp;= c^T_{\\mathcal{B}} (\\mathcal{B}^{-1})_{\\_i} \\\\ \\end{align}\\] 11.4 Dual variables By strong duality, we know that the primal objective value equals the dual objective value i.e.  \\[\\begin{align} \\mathbb{O} &amp;= b_1 y_1^* + \\cdots + b_m y_m^* \\end{align}\\] So, we get \\(\\partial \\mathbb{O}/\\partial b_i = y_i^*\\). Because of this result, \\(y_i^*\\) is also called the shadow price or the marginal cost of the \\(i^{th}\\) constraint. Combining this with the rate of change calculation above, we get \\[\\begin{align} y^*_i = c^T_{\\mathcal{B}} (\\mathcal{B}^{-1})_{\\_i}. \\end{align}\\] We can combine all the above coordinates into a single vector to get the following result. Theorem 11.1 For a non-degenerate linear program, the dual optimal solution is given by \\((\\mathcal{B}^{-1})^Tc_{\\mathcal{B}}\\). This theorem provides yet another method of finding the dual optimal solution without having to solve the dual linear program. Example 11.4 For the linear program (1.2), the objective function is \\[\\begin{align} \\mathbb{O} &amp; = 4 x + 3y \\\\ &amp; = 4x + 3y + 0 w_3 + 0 w_1 + 0 w_2 \\end{align}\\] So, \\(c_{\\mathcal{B}} = \\begin{bmatrix}4 \\\\ 3 \\\\ 0 \\end{bmatrix}\\) and \\(c_\\mathcal{N} = \\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\\). Using the value of \\(\\mathcal{B}^{-1}\\) calculated above, we get \\[\\begin{align} y^* &amp; = (\\mathcal{B}^{-1})^Tc_{\\mathcal{B}} \\\\ &amp; = \\begin{bmatrix} -1/9 &amp; 2/9 &amp; -1/9 \\\\ 2/3 &amp; -1/3 &amp; -1/3 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 3 \\\\ 0 \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} 2/9 \\\\ 5/3 \\\\ 0 \\end{bmatrix}. \\end{align}\\] To check that this is indeed dual-optimal, we calculate the dual-objective value at this solution \\[\\begin{align} b^T y^* &amp; = 3.6 (2/9) + 1.5 (5/3) + 0 (1) \\\\ &amp;= 3.3 \\end{align}\\] which equals the optimal objective value of the primal. One can check that this solution is also dual-feasible and hence is the dual-optimal solution by Certificate of Optimality (Theorem 9.1). "],["sensitivity-analysis---objective.html", "Chapter 12 Sensitivity analysis - Objective", " Chapter 12 Sensitivity analysis - Objective We can ask the same questions as in the previous chapter about the change in the objective coefficients - how far can we change the objective coefficient \\(c_i\\) without changing the set of basic and non-basic variables at the optimal solution. Note that in this case we are not changing the constraints and therefore the feasible region. So, this is the same as asking - how far can we change the objective coefficient \\(c_i\\) without changing the optimal solution. We can redo the entire analysis for the objective coefficients from scratch. However, we also know the objective coefficients are the constraint upper bounds for the dual linear program and by Strong duality (Theorem 8.2) the primal and dual optimal objective values are the same. Thus performing sensitivity analysis on the objective coefficients of the primal is the same as performing sensitivity analysis on the constraints of the dual. Consider the standardized dual \\[\\begin{equation} \\begin{array}{lrll} \\mbox{maximize: } &amp; -b^T y \\\\ \\mbox{subject to: } &amp; -A^T y &amp; \\leq &amp; -c \\\\ &amp; y &amp; \\geq &amp; 0, \\end{array} \\end{equation}\\] Using Equation (11.3) for this dictionary we get the range of optimality for \\(-c_j\\) to be \\[\\begin{equation} (-\\mathcal{B}_d)^{-1} (-c) + \\delta (-\\mathcal{B}_d^{-1})_{\\_j} \\ge 0. \\end{equation}\\] where \\(-\\mathcal{B}_d\\) is formed by combining the dual-basic columns of \\(\\begin{bmatrix} -A^T &amp; I_n\\end{bmatrix}\\) and so \\(\\mathcal{B}_d\\) is formed by combining the dual-basic columns of \\(\\begin{bmatrix} A^T &amp; -I_n\\end{bmatrix}\\). This simplifies to \\[\\begin{equation} \\mathcal{B}_d^{-1} c - \\delta (\\mathcal{B}_d^{-1})_{\\_j} \\ge 0. \\end{equation}\\] Note that this is the range of optimality for \\(-c_j\\). To get the range of optimality for \\(c_j\\) we need to replace \\(\\delta\\) by \\(-\\delta\\) to get \\[\\begin{equation} \\mathcal{B}_d^{-1} c + \\delta (\\mathcal{B}_d^{-1})_{\\_j} \\ge 0. \\end{equation}\\] Finally, by Lemma 11.1 \\(\\mathcal{B}_d^{-1} c\\) is the vector of values of the dual basic variables. Hence we get, the range of optimality for \\(c_j\\) is the interval \\((c_j + \\delta_-, c_j + \\delta_+)\\) such that \\(y_{\\mathcal{B}}^* + \\delta (\\mathcal{B}_d^{-1})_{\\_j} \\ge 0\\) for all \\(\\delta \\in (\\delta_-, \\delta_+)\\). Note that as a happy accident all the negative signs cancel out and the equation for finding the range of optimality for the objective coefficients is exactly the same as the one for finding the range of optimality for the constraints. Theorem 12.1 The range of optimality for the constraints and the objective functions are given by the following formulae: Range of optimality for \\(b_i\\): \\[\\begin{equation} x_{\\mathcal{B}}^* + \\delta (\\mathcal{B}^{-1})_{\\_i} \\ge 0. \\end{equation}\\] where \\(\\mathcal{B}\\) is formed by combining the primal-basic columns of \\(\\begin{bmatrix} A &amp; I_m \\end{bmatrix}\\), and Range of optimality for \\(c_j\\): \\[\\begin{equation} y_{\\mathcal{B}}^* + \\delta (\\mathcal{B}_d^{-1})_{\\_j} \\ge 0. \\end{equation}\\] where \\(\\mathcal{B}_d\\) is formed by combining the dual-basic columns of \\(\\begin{bmatrix} A^T &amp; -I_n \\end{bmatrix}\\). Finally, to find \\(\\mathcal{B}_d\\) we note that, by Complementary Slackness (Theorem 9.2), if the linear program is non-degenerate then the dual basic variables correspond to the primal non-basic variables (as the dual basic variables must be non-zero). This statement is true even for degenerate linear programs but in this case the proof is more subtle and requires the use of Strong duality (Theorem 8.2). Example 12.1 Consider the linear program (1.2) again. At the optimal solution, as \\(w_1\\) and \\(w_2\\) are non-basic, the corresponding dual variables (\\(y_1\\) and \\(y_2\\)) will be basic for the dual linear program (7.2). This gives us \\[\\begin{align} \\mathcal{B}_d &amp;= \\begin{bmatrix} 3 &amp; 2 \\\\ 6 &amp; 1 \\end{bmatrix} \\\\ \\implies \\mathcal{B}_d^{-1} &amp;= \\begin{bmatrix} -1/9 &amp; 2/9 \\\\ 2/3 &amp; -1/3 \\end{bmatrix}. \\end{align}\\] From Example 11.4 we know that \\(y_1 = 2/9\\) and \\(y_2 = 5/3\\) at the dual optimal solution. Using these, we can now find the range of optimality for the objective coefficients. To find the range of optimality for \\(c_1 = 4\\) we solve \\[\\begin{align} \\begin{bmatrix} 2/9 \\\\ 5/3 \\end{bmatrix} + \\delta \\begin{bmatrix} -1/9 \\\\ 2/3 \\end{bmatrix} \\ge 0 \\end{align}\\] which gives us the inequalities \\[\\begin{align} \\begin{array}{lrlrrll} 2/9 + \\delta (-1/9) &amp; \\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\le &amp; 2 \\\\ 5/3 + \\delta (2/3) &amp; \\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\ge &amp; -5/2 \\end{array} \\end{align}\\] So, \\(\\delta_- = -5/2\\) and \\(\\delta_+ = 2\\) and the range of optimality for \\(c_1\\) is \\((4 - 5/2, 4 + 2) = (1.5, 6)\\). To find the range of optimality for \\(c_2 = 3\\) we solve \\[\\begin{align} \\begin{bmatrix} 2/9 \\\\ 5/3 \\end{bmatrix} + \\delta \\begin{bmatrix} 2/9 \\\\ -1/3 \\end{bmatrix} \\ge 0 \\end{align}\\] which gives us the inequalities \\[\\begin{align} \\begin{array}{lrlrrll} 2/9 + \\delta (2/9) &amp; \\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\ge &amp; -1 \\\\ 5/3 + \\delta (-1/3) &amp; \\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\le &amp; 5 \\end{array} \\end{align}\\] So, \\(\\delta_- = -1\\) and \\(\\delta_+ = 5\\) and the range of optimality for \\(c_1\\) is \\((3 - 1, 3 + 5) = (2, 8)\\). "],["convexity.html", "Chapter 13 Convexity 13.1 Convex programming 13.2 Boundary", " Chapter 13 Convexity We will now move toward generalizing the concepts from linear programming to non-linear optimization questions. Broadly speaking, we have studied three different topics so far: Simplex method, Duality theory, and Sensitivity analysis. Of these the simplex method is hardest to generalize to non-linear optimization while sensitivity analysis is the easiest.1 In fact, sensitivity analysis is important for any modeling question (not just optimization problems) and the general techniques for it lie beyond the scope of this course. This leaves us with duality theory. The class of optimization to which duality theory naturally extends is convex optimization problems. 13.1 Convex programming We start by defining convex sets, convex functions, closed sets, and convex optimization problems. We say that a subset \\(\\mathcal{S}\\) of \\(\\mathbb{R}^n\\) is called convex if for any points \\({x}_1\\) and \\({x}_2\\) in \\(\\mathcal{S}\\), the line segment connecting them also lies in \\(\\mathcal{S}\\). More precisely, we say that \\(S\\) is convex if forall \\({x}_1\\) in \\(\\mathcal{S}\\), \\({x}_2\\) in \\(\\mathcal{S}\\), and \\(t \\in [0,1]\\), the point \\((1 - t) {x}_1 + t {x}_2\\) also lies in \\(\\mathcal{S}\\). We say that function \\(f : \\mathcal{S} \\to \\mathbb{R}\\) is convex if for all \\({x}_1\\) in \\(\\mathcal{S}\\), \\({x}_2\\) in \\(\\mathcal{S}\\), and \\(t \\in [0,1]\\), the inequality \\(f((1 - t) {x}_1 + t {x}_2) \\le (1 - t) f({x}_1) + t f({x}_2)\\) holds. Geometrically, this is saying that any line segment connecting two points on the graph of \\(f\\) lies above the graph. The following theorem provides a quick way to check if a function is convex and to come up with examples of convex functions. Theorem 13.1 Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be a \\(C^2\\) function. \\(f\\) is convex if and only if the Hessian matrix of \\(f\\) is positive semi-definite. In particular, when \\(n = 1\\), \\(f\\) is convex if and only if \\(f&#39;&#39; \\ge 0\\). We say that a set is closed if contains all of its limit points. The precise definition of closedness is beyond the scope of this class. Geometrically, a set is closed if it contains its boundary. For example, the set \\([0,1]\\) is a closed subset of \\(\\mathbb{R}^1\\) whereas the set \\([0,1)\\) is not. Finally, an optimization problem \\[\\begin{equation} \\begin{array}{ll} \\mbox{maximize: } &amp; f(x) \\\\ \\mbox{subject to: } &amp; x \\in \\mathcal{S} \\end{array} \\tag{13.1} \\end{equation}\\] is called convex if \\(\\mathcal{S}\\) is a closed and convex subset of \\(\\mathbb{R}^n\\) and \\(f : \\mathcal{S} \\to \\mathbb{R}\\) is a convex function. The following theorem is easy to prove using basic definitions. Theorem 13.2 A linear program is a convex optimization problem. Remark. From now on, we’ll let \\(\\mathcal{S}\\) denote a closed and convex subset of \\(\\mathbb{R}^n\\) and let \\(f\\) denote a convex function from \\(\\mathcal{S}\\) to \\(\\mathbb{R}\\). 13.2 Boundary Theorem 13.3 The convex optimization problem (13.1) either has an optimal solution on the boundary of \\(\\mathcal{S}\\) or is unbounded. Proof. Suppose \\({x}\\) is a point in the interior of \\(\\mathcal{S}\\). It suffices to show that either there is some point on the boundary of \\(\\mathcal{S}\\) with an objective value that is greater than or equal to the objective value of \\({x}\\) or the problem is unbounded. Let \\({x}_1\\) be any point on the boundary of \\(\\mathcal{S}\\). Draw a ray starting at \\({x}_1\\) in the direction of \\({x}\\). As \\(\\mathcal{S}\\) is closed and convex, two cases are possible: The ray intersects the boundary of \\(\\mathcal{S}\\) in exactly one more point, say \\({x}_2\\). The ray does not intersect the boundary of \\(\\mathcal{S}\\) and the entire ray is contained within \\(\\mathcal{S}\\). Case 1: We will show that either \\(f({x}) \\le f({x}_1)\\) or \\(f({x}) \\le f({x}_2)\\). We prove this by contradiction. Suppose \\(f({x}) &gt; f({x}_1)\\) and \\(f({x}) &gt; f({x}_2)\\). Multiplying the first inequality by \\((1-t)\\) and the second by \\(t\\) and adding them together, we get \\[\\begin{align} &amp;&amp; (1-t) f({x}) + t f({x}) &amp;&gt; (1-t) f({x}_1) + t f({x}_2) \\\\ \\implies &amp;&amp; f({x}) &amp;&gt; (1-t) f({x}_1) + t f({x}_2) . \\end{align}\\] But this contradicts the convexity of \\(f\\). Case 2: If \\(f(x_1) \\ge f(x)\\), we’re done. Suppose this is not the case. We’ll show that the problem is unbounded. For \\(0 &lt; t \\le 1\\), let \\({y}_t\\) denote the point on the ray such that \\[\\begin{align} {x} = (1-t) {x}_1 + t {y}_t. \\end{align}\\] For example, \\({y}_1 = {x}\\) and \\({y}_{1/2}\\) is the point on the ray for which \\({x}\\) is halfway between \\({x}_1\\) and \\({y}_{1/2}\\). As \\(t\\) decreases, \\({y}_t\\) will move farther and farther away from \\({x}\\). Note that all of \\({y}_t\\) lie inside \\(\\mathcal{S}\\). By convexity of \\(f\\), we know that \\[\\begin{align} &amp;&amp; f({x}) &amp; \\le (1-t) f({x}_1) + t f({y}_ t) \\\\ \\implies &amp;&amp; f({x}) - (1-t) f({x}_1) &amp;\\le t f({y}_t) \\\\ \\implies &amp;&amp; t^{-1}f({x}) - (t^{-1}-1) f({x}_1) &amp;\\le f({y}_t) \\\\ \\implies &amp;&amp; t^{-1}(f({x}) - f({x}_1)) + f({x}_1) &amp;\\le f({y}_t) \\\\ \\end{align}\\] As \\(f({x}) &gt; f({x}_1)\\), the left hand side tends to \\(\\infty\\) as \\(t\\) decreases and the optimization problem is unbounded. The following corollary follows by applying Extreme Value Theorem to the above result. Theorem 13.4 If \\(\\mathcal{S}\\) is bounded (in addition to being closed and convex), then the convex optimization problem (13.1) has an optimal solution on the boundary of \\(\\mathcal{S}\\). Other techniques for solving linear programs like the ellipsoid algorithms and interior point methods do generalize to non-linear programs.↩︎ "],["convexity-and-duality.html", "Chapter 14 Convexity and duality 14.1 Farkas’ lemma 14.2 Geometry 14.3 Convex polyhedra 14.4 Equivalence with Strong Duality", " Chapter 14 Convexity and duality We’ll next prove a couple of geometric results that are equivalent to strong duality theorem (Theorem 8.2). 14.1 Farkas’ lemma Theorem 14.1 (Farkas' lemma) Let \\(A\\) be an \\(m \\times n\\) matrix and \\(b\\) be a vector in \\(\\mathbb{R}^m\\). Exactly one of the following systems has a solution: \\[\\begin{align} A x &amp; = b \\\\ x &amp; \\ge 0. \\tag{14.1} \\end{align}\\] \\[\\begin{align} y^T A &amp; \\ge 0 \\\\ y^T b &amp; &lt; 0. \\tag{14.2} \\end{align}\\] Proof. We’ll prove Farkas’ lemma using strong duality. Consider the following linear program: \\[\\begin{equation} \\begin{array}{llll} \\mbox{maximize: } &amp; 0 \\\\ \\mbox{subject to: } &amp; A x &amp; = &amp; b \\\\ &amp; x &amp; \\ge &amp; 0. \\end{array} \\tag{14.3} \\end{equation}\\] The optimal solution to the linear program (14.3) is a feasible solution to (14.1). The dual to this linear program is \\[\\begin{equation} \\begin{array}{llll} \\mbox{minimize: } &amp; y^T b \\\\ \\mbox{subject to: } &amp; y^T A &amp; \\ge &amp; 0. \\end{array} \\tag{14.4} \\end{equation}\\] Case 1: Suppose (14.1) has a solution. In this case, (14.3) has an optimal solution. By strong duality, (14.4) also has an optimal solution with optimal objective \\(0\\). So, the minimum value of \\(y^T b\\) is \\(0\\) and hence the system (14.2) does not have a solution. Case 2: Suppose (14.1) does not have a solution. In this case, (14.3) has no optimal solution. By strong duality, neither does (14.4). So, (14.4) is either infeasible or unbounded. As \\(y = 0\\) is a feasible solution to (14.4) it cannot be infeasible, and hence it must be unbounded. But this means that the value of \\(y^T b\\) can be made arbitrarily small, and in particular, can be made negative. Hence, the system (14.2) has a solution. We’ll next interpret Farkas’ lemma geometrically using positive cones and separating hyperplanes. 14.2 Geometry 14.2.1 Convex cones Define the convex cone, also called the positive cone, of a finite set of vectors in \\(\\mathbb{R}^m\\) to be the set of positive linear combinations of vectors in the set.2 \\[\\begin{align} C_+(v_1, \\dots, v_n) &amp; := \\{c_1 v_1 + \\dots + c_n v_n \\mid c_i \\ge 0 \\}. \\end{align}\\] 14.2.2 Hyperplanes and Half-spaces A hyperplane in \\(\\mathbb{R}^m\\) is the set of solutions to a single linear equation. The complement of a hyperplane in \\(\\mathbb{R}^n\\) consists of two connected components. The closures of these components are called half-spaces. If the equation of the hyperplane is given by \\(b^T y = b_0\\), where \\(b\\) is a vector in \\(\\mathbb{R}^m\\) and \\(b_0 \\in \\mathbb{R}\\), then the corresponding two half-spaces are described by \\(b^T y \\le b_0\\) and \\(b^T y \\ge b_0\\). We say that a hyperplane \\(H\\) separates two subsets \\(S_1\\) and \\(S_2\\) of \\(\\mathbb{R}^m\\) if \\(S_1\\) and \\(S_2\\) belongs to the two different half-spaces of \\(H\\). 14.2.3 Geometric version of Farkas’ lemma Using convex cones and separating hyperplanes, we can reinterpret 14.1 as follows. Theorem 14.2 (Geometric version of Farkas' lemma) Let \\(v_1, \\dots, v_n, b\\) be vectors in \\(\\mathbb{R}^m\\). Exactly one of the following statements is true: Either \\(b\\) lies inside \\(C_+(v_1, \\dots, v_n)\\), or There is a hyperplane \\(H\\) that separates \\(b\\) from \\(C_+(v_1, \\dots, v_n)\\). As we’ll see below, this is a special case of the Separating Hyperplane Theorem 14.3. Based on these theorems, we can design algorithms that search for a separating hyperplane when trying to separate convex regions. See Support Vector Machines for an example of such an algorithm. 14.3 Convex polyhedra A point \\(b\\) and a convex cone \\(C_+(v_1, \\dots, v_n)\\) are convex subsets of \\(\\mathbb{R}^m\\). The statement of Farkas’ lemma 14.2 can be generalized to arbitrary convex sets using metric topology. For now, we’ll generalize it to convex polyhedra. Intersection of finitely many half-spaces is called a convex polyhedron. So, a convex polyhedron is the set of solutions to a system of linear inequalities \\(A x \\le b\\). But this set is precisely the feasible region of a linear program! The class of convex polyhedra is very large and all geometric “linear” convex objects, like points, lines, planes, half-spaces, hyperplanes, convex cones, etc. can be realized as convex polyhedra. The following is an extension of Farkas’ lemma to convex polyhedra. Theorem 14.3 (Separating Hyperplane Theorem) Let \\(P\\) and \\(P&#39;\\) be two non-empty, disjoint, convex polyhedra in \\(\\mathbb{R}^m\\). There exists a half-plane \\(H\\) that separates \\(P\\) and \\(P&#39;\\). Proof. We’ll show the following stronger statement: there exists a vector \\(c \\in \\mathbb{R}^m\\) and constants \\(d &lt; d&#39;\\) such that \\(P\\) is a subset of the hyperplane \\(c^T y \\le d_1\\) and \\(P&#39;\\) is a subset of the hyperplane \\(c^T y \\ge d_2\\). We can then choose our separating hyperplane to be \\(c^T y = (d + d&#39;)/2\\). Suppose \\(P\\) and \\(P&#39;\\) are described by the equations \\(A x \\le b\\) and \\(A&#39;x \\le b&#39;\\), respectively. Then because the two sets are disjoint, the following system does not have a solution: \\[\\begin{align} A x &amp;\\le b \\\\ A&#39;x &amp;\\le b&#39; \\end{align}\\] which can be rewritten as \\[\\begin{align} \\begin{bmatrix} A \\\\ A&#39; \\end{bmatrix} x &amp; \\le \\begin{bmatrix} b \\\\ b&#39; \\end{bmatrix}. \\end{align}\\] We can now apply (a variant of) Farkas’ lemma (Theorem 14.1) to this system to extract \\(c\\), \\(d\\), \\(d&#39;\\) from solutions from the dual system. 14.4 Equivalence with Strong Duality It is possible to prove Strong Duality using Farkas’ lemma, which itself can be proven using metric topology. So, Strong Duality, Farkas’ lemma, and Separating Hyperplane Theorem should all be thought of as equivalent to each other. Proof. Consider the system of equations \\[\\begin{equation} \\begin{split} Ax &amp; \\le b \\\\ -A^T y &amp; \\le -c \\\\ x, y &amp; \\ge 0 \\\\ -c^T x + b^T y &amp; \\le 0. \\end{split} \\tag{14.5} \\end{equation}\\] Suppose this system has a feasible solution. The first three equations are equivalent to saying that \\(x\\) is primal-feasible and \\(y\\) is dual-feasible. If such a solution exists, by Weak Duality (Theorem 8.1) we know that \\(c^T x \\le b^T y\\). So, the only way the fourth inequality is satisfied is if \\(c^T x = b^T y\\) i.e. the primal-objective value at \\(x\\) equals the dual-objective value at \\(y\\). But by Weak Duality, these must then be the optimal solutions thereby proving Strong Duality. So, it suffices to show that (14.5) has a feasible solution if the primal has an optimal solution. We prove this by contradiction. Suppose (14.5) does not have an optimal solution. We rewrite the system as \\[\\begin{equation} \\begin{split} \\begin{bmatrix} A &amp; 0 \\\\ 0 &amp; -A^T \\\\ -c^T &amp; b^T \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} &amp; \\le \\begin{bmatrix} b \\\\ -c \\\\ 0 \\end{bmatrix} \\\\ x, y &amp; \\ge 0. \\end{split} \\end{equation}\\] By (a variant of) Farkas’ lemma, the corresponding dual system must have a solution. We then analyze the solution to the dual system to show that this is not possible. One can also define convex cones for infinite sets of vectors. In this case, we need to take the closure of the set of positive linear combinations.↩︎ "],["interior-point-methods.html", "Chapter 15 Interior point methods 15.1 Gradient descent 15.2 Barrier functions", " Chapter 15 Interior point methods Throughout this chapter we will let \\(f\\) denote a twice differentiable function from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\). 15.1 Gradient descent We will start by trying to solve the following unconstrained optimization problem: \\[\\begin{align} \\mbox{minimize: } &amp; f(x) \\end{align}\\] where \\(x\\) is any vector in \\(\\mathbb{R}^n\\). Gradient descent is a simple algorithm for solving this problem using basic differential calculus. Gradient descent relies on the fact that the negative of the gradient points in the direction in which the function decreases the fastest. So the general principle is to move in the direction of the negative gradient until the function is no longer decreasing. More precisely, Let \\(k = 0\\). Start with a guess \\(x_0\\) for the optimal solution. While minima is not reached: Set \\(x_{k+1} = x_k - t_k \\nabla f(x_k)\\). Increase \\(k\\) to \\(k+1\\). Here \\(t_k\\) is the “step size” for the \\(k^{th}\\) iteration and can be chosen to be a small constant or some function of \\(k\\), \\(x_k\\), \\(f(x_k)\\), or \\(\\nabla f(x_k)\\). There are several issues with the above algorithm: If the function has multiple local minima, then the algorithm might converge to a non-absolute minima depending on the starting guess and the choice of step sizes. If the step sizes are chosen to be too large, then the algorithm can completely miss the minima and may not converge. If the step sizes are chosen to be too small, then the algorithm may take a long time to converge. Unfortunately, there is no easy way to resolve these issues. In practice, either we need to be able to control the function and its gradient or we proceed by trial and error to find step sizes that work. Even then there is no guarantee that the algorithm will converge to an absolute minima and not a local minima, if at all. In spite of these issues, because of its simplicity, ease of implementation, and good convergence properties in practice, Gradient Descent is a very popular algorithm for solving unconstrained optimization problems. 15.2 Barrier functions Gradient descent algorithm can be modified to solve constrained optimization problems. Consider the following problem: \\[\\begin{equation} \\begin{array}{llr} \\mbox{minimize: } &amp; f(x) \\\\ \\mbox{subject to: } &amp; g_i(x) \\ge 0, &amp; \\mbox{ for } 1 \\le i \\le m. \\\\ \\end{array} \\tag{15.1} \\end{equation}\\] We can apply GD to this problem and find a critical point for \\(f(x)\\). However, this might not answer the optimization question for two reasons: The critical point might not be in the feasible region. The optimal solution might not be obtained at a critical of \\(f(x)\\) and could lie on the boundary, as is the case for linear programming. The issue is that the gradient function algorithm only sees the objective function and does not know the constraints. So, we modify the objective function to include the constraints using barrier functions. A barrier function is a differentiable function \\(b\\) from \\((0, \\infty)\\) to \\(\\mathbb{R}\\) that has the property \\(\\lim \\limits_{x \\to 0^+} f(x) = \\infty\\). We’ll use the barrier function \\(-\\ln x\\). Using a small positive parameter \\(\\mu\\), we create a new objective function: \\[\\begin{align} f_\\mu(x) := &amp; f(x) - \\mu \\sum \\limits_{i = 1} ^ m \\ln (g_i(x)). \\end{align}\\] Because the domain of \\(\\ln(x)\\) is \\((0, \\infty)\\), any critical point of \\(f_\\mu(x)\\) must lie in the feasible region of (15.1). Moreover, if the following unconstrained optimization problem has an optimal solution then it will be away from the boundary of (15.1) as the cost of approaching the boundary goes to \\(\\infty\\): \\[\\begin{align} \\mbox{minimize: } &amp; f_\\mu (x). \\end{align}\\] Let \\(x_\\mu^*\\) be a solution of the above problem. We’ll make the following assumption without proof: \\(x_\\mu^*\\) exists and is continuous in \\(\\mu\\) and \\(\\lim \\limits_{\\mu \\to 0^+} x_\\mu^* = x^*\\) where \\(x^*\\) is an optimal solution to (15.1). This assumption is valid, for example, when \\(f(x)\\) and \\(g_i(x)\\) are convex functions and the feasible region has an interior. With this assumption, we now have a method for solving (15.1): Start with a small \\(\\mu_0 &gt; 0\\) and optimize \\(f_{\\mu_0}(x)\\) using GD. Call the solution \\(x_{\\mu_0}^*\\). Repeat the following until the sequence \\(x_{\\mu_k}^*\\) stabilizes sufficiently: Set \\(\\mu_{k+1} = \\mu_k - \\delta_k\\) for some small \\(\\delta_k\\). Using \\(x^*_{\\mu_k}\\) as a starting point for GD, optimize \\(f_{\\mu_{k+1}}(x)\\). Call the solution \\(x_{\\mu_{k+1}}^*\\). Increase \\(k\\) to \\(k + 1\\). This method is a very simplified interior point method. The sequence of points \\(x_{\\mu}\\) is called the central path. Example 15.1 We can compute the central path explicitly for some simple examples. Consider the optimization problem: \\[\\begin{align} \\mbox{minimize: } &amp; (x + 1)^2 \\\\ \\mbox{subject to: } &amp; x \\ge 0. \\end{align}\\] It is easy to see that \\(x^* = 0\\) is the optimal solution. We can calculate the central path as follows: \\[\\begin{align} f_\\mu(x) := (x + 1)^2 - \\mu \\ln (x). \\end{align}\\] The critical points for this function can be obtained by setting the derivative to 0. \\[\\begin{align} f&#39;_\\mu(x) &amp;= 0 \\\\ \\implies 2(x + 1) - \\dfrac{\\mu}{x} &amp;= 0 \\\\ \\implies x^2 + x - \\mu/2 &amp;= 0 \\\\ \\implies x &amp;= \\dfrac{-1 \\pm \\sqrt{1 + 2 \\mu}}{2}. \\end{align}\\] Only one of the two satisfy \\(x \\ge 0\\), so we get \\[\\begin{align} x_\\mu^* = \\dfrac{-1 + \\sqrt{1 + 2 \\mu}}{2}. \\end{align}\\] This is the central path. Taking the limit as \\(\\mu \\to 0\\) we get, \\[\\begin{align} \\lim \\limits_{\\mu \\to 0^+} x_\\mu^* &amp; = \\lim \\limits_{\\mu \\to 0^+} \\dfrac{-1 + \\sqrt{1 + 2 \\mu}}{2} \\\\ &amp;= 0 \\\\ &amp;= x^*. \\end{align}\\] "],["kkt-conditions.html", "Chapter 16 KKT conditions 16.1 KKT conditions", " Chapter 16 KKT conditions We can use the central path to derive necessary conditions for optimality. Consider the optimization problem (15.1) again. As before, we’ll assume that there is a central path \\(x_\\mu^*\\) converging to an optimal solution \\(x^*\\). Because \\(x_\\mu^*\\) is a critical point of \\(f_\\mu(x)\\), we get \\[\\begin{align} \\nabla_x f_\\mu(x_\\mu^*) &amp;= 0 \\\\ \\implies \\nabla f(x_\\mu^*) - \\sum \\limits_{i = 1}^m \\dfrac{\\mu}{g_i(x_\\mu^*)} \\nabla g_i (x_\\mu^*)&amp;= 0 \\end{align}\\] Applying \\(\\lim \\limits_{\\mu \\to 0^+}\\) to both sides, we get \\[\\begin{align} \\lim \\limits_{\\mu \\to 0^+} \\nabla f(x_\\mu^*) - \\sum \\limits_{i = 1}^m \\lim \\limits_{\\mu \\to 0^+}\\dfrac{\\mu}{g_i(x_\\mu^*)} \\nabla g_i (x_\\mu^*)&amp;= 0 \\end{align}\\] which simplifies to \\[\\begin{align} \\nabla f(x^*) = \\sum \\limits_{i = 1}^m \\lambda_i \\nabla g_i (x^*) \\end{align}\\] where \\(\\lambda_i = \\lim \\limits_{\\mu \\to 0^+}\\frac{\\mu}{g_i(x_\\mu^*)}\\) for \\(1 \\le i \\le m\\). (There are several assumptions here about the existence and convergence of limits that we’re brushing under the rug.) We can further analyze the constants \\(\\lambda_i\\). \\[\\begin{align} &amp;&amp; \\dfrac{\\mu}{g_i(x_\\mu^*)} \\cdot g_i(x_\\mu^*) &amp;= \\mu \\\\ \\implies &amp;&amp; \\lim \\limits_{\\mu \\to 0^+}\\dfrac{\\mu}{g_i(x_\\mu^*)} \\cdot \\lim \\limits_{\\mu \\to 0^+}g_i(x_\\mu^*) &amp;= \\lim \\limits_{\\mu \\to 0^+}\\mu \\\\ \\implies &amp;&amp;\\lambda_i g_i(x^*) &amp;= 0. \\end{align}\\] Finally, because \\(g_i(x_\\mu^*) \\ge 0\\) and \\(\\mu &gt; 0\\), we must have \\[\\begin{align} \\lambda_i \\ge 0. \\end{align}\\] To summarize, if \\(x^*\\) is an optimal solution to (15.1) and there exists a central path \\(x_\\mu^*\\) converging to \\(x^*\\), then the following conditions are satisfied: \\[\\begin{align} \\nabla f(x^*) &amp;= \\sum \\limits_{i = 1}^m \\lambda_i \\nabla g_i (x^*) \\\\ \\lambda_i g_i(x^*) &amp;= 0 \\\\ \\lambda_i &amp;\\ge 0. \\end{align}\\] These are called the KKT conditions. 16.1 KKT conditions We can combine the above conditions with Lagrange multipliers to get the following slight generalization: Theorem 16.1 (KKT conditions) Consider the optimization problem: \\[\\begin{equation} \\begin{array}{llr} \\mbox{minimize: } &amp; f(x) \\\\ \\mbox{subject to: } &amp; g_i(x) \\ge 0, &amp; \\mbox{ for } 1 \\le i \\le m \\\\ &amp; h_i(x) = 0, &amp; \\mbox{ for } 1 \\le i \\le k. \\end{array} \\end{equation}\\] Assume that this problem has an optimal solution \\(x^*\\) and there is a central path \\(x_\\mu^*\\) converging to \\(x^*\\). Then under certain regularity conditions, the following conditions are satisfied: \\[\\begin{align} \\nabla f(x^*) &amp;= \\sum \\limits_{i = 1}^m \\lambda_i \\nabla g_i (x^*) + \\sum \\limits_{i = 1}^k \\lambda&#39;_i \\nabla h_i (x^*) \\\\ \\lambda_i g_i(x^*) &amp;= 0, \\mbox{ for } 1 \\le i \\le m \\\\ \\lambda_i &amp;\\ge 0, \\mbox{ for } 1 \\le i \\le m. \\end{align}\\] The conditions for convergence and the regularity conditions are quite non-trivial and requires us to study the Lagrangian dual of the optimization problem. Example 16.1 Consider Example 15.1 again. The KKT conditions for this example are \\[\\begin{align} 2(x + 1) &amp;= \\lambda \\\\ \\lambda x &amp;= 0 \\\\ \\lambda &amp;\\ge 0. \\end{align}\\] These need to be satisfied in addition to the original constraint \\(x \\ge 0\\). The only solution to this system is \\(x = 0\\), which is indeed the optimal solution. Example 16.2 Consider the linear program \\[\\begin{align} \\mbox{minimize: } &amp; c^T x \\\\ \\mbox{subject to: } &amp; Ax \\ge b \\end{align}\\] The KKT conditions for this problem are \\[\\begin{align} c &amp;= A^T \\lambda \\\\ x_i \\lambda _i &amp;= 0, \\mbox{ for } 1 \\le i \\le n,\\\\ \\lambda &amp;\\ge 0. \\end{align}\\] This is precisely the dual of the linear programming problem and complementary slackness condition. Thus the KKT conditions recover the duality in the linear programming sense. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
