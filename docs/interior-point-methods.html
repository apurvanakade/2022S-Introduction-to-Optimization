<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Interior point methods | Introduction to Optimization</title>
  <meta name="description" content="Chapter 15 Interior point methods | Introduction to Optimization." />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Interior point methods | Introduction to Optimization" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 15 Interior point methods | Introduction to Optimization." />
  <meta name="github-repo" content="apurvanakade/2022S-Introduction-to-optimization" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Interior point methods | Introduction to Optimization" />
  
  <meta name="twitter:description" content="Chapter 15 Interior point methods | Introduction to Optimization." />
  

<meta name="author" content="Apurva Nakade" />


<meta name="date" content="2022-05-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="convexity-and-duality.html"/>
<link rel="next" href="kkt-conditions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="index.html#preface">Preface<span></span></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#software-tools"><i class="fa fa-check"></i><b>1.1</b> Software tools<span></span></a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#graphing-calculator"><i class="fa fa-check"></i><b>1.1.1</b> Graphing calculator<span></span></a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#solver-add-in"><i class="fa fa-check"></i><b>1.1.2</b> Solver add-in<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="standard-linear-program.html"><a href="standard-linear-program.html"><i class="fa fa-check"></i><b>2</b> Standard linear program<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="standard-linear-program.html"><a href="standard-linear-program.html#slack-variables"><i class="fa fa-check"></i><b>2.1</b> Slack variables<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="standard-linear-program.html"><a href="standard-linear-program.html#basic-and-non-basic-variables"><i class="fa fa-check"></i><b>2.2</b> Basic and non-basic variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simplex-method.html"><a href="simplex-method.html"><i class="fa fa-check"></i><b>3</b> Simplex method<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="simplex-method.html"><a href="simplex-method.html#entering-and-leaving-variables"><i class="fa fa-check"></i><b>3.1</b> Entering and leaving variables<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="simplex-method.html"><a href="simplex-method.html#dictionaries"><i class="fa fa-check"></i><b>3.2</b> Dictionaries<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="simplex-method.html"><a href="simplex-method.html#the-simplex-step"><i class="fa fa-check"></i><b>3.3</b> The simplex step<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="simplex-method.html"><a href="simplex-method.html#choosing-the-entering-variable"><i class="fa fa-check"></i><b>3.3.1</b> Choosing the entering variable<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="simplex-method.html"><a href="simplex-method.html#choosing-the-leaving-variable"><i class="fa fa-check"></i><b>3.3.2</b> Choosing the leaving variable<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="simplex-method.html"><a href="simplex-method.html#tableau"><i class="fa fa-check"></i><b>3.3.3</b> Tableau<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="simplex-method.html"><a href="simplex-method.html#stopping-conditions"><i class="fa fa-check"></i><b>3.4</b> Stopping conditions<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="simplex-method.html"><a href="simplex-method.html#no-entering-variable"><i class="fa fa-check"></i><b>3.4.1</b> No entering variable<span></span></a></li>
<li class="chapter" data-level="3.4.2" data-path="simplex-method.html"><a href="simplex-method.html#no-leaving-variable"><i class="fa fa-check"></i><b>3.4.2</b> No leaving variable<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="initialization.html"><a href="initialization.html"><i class="fa fa-check"></i><b>4</b> Initialization<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="initialization.html"><a href="initialization.html#auxiliary-linear-program"><i class="fa fa-check"></i><b>4.1</b> Auxiliary linear program<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="initialization.html"><a href="initialization.html#combined-tableau"><i class="fa fa-check"></i><b>4.2</b> Combined tableau<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="halting-problem.html"><a href="halting-problem.html"><i class="fa fa-check"></i><b>5</b> Halting problem<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="halting-problem.html"><a href="halting-problem.html#fundamental-theorem-of-linear-programming"><i class="fa fa-check"></i><b>5.1</b> Fundamental theorem of linear programming<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="halting-problem.html"><a href="halting-problem.html#degeneracy"><i class="fa fa-check"></i><b>5.2</b> Degeneracy<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="halting-problem.html"><a href="halting-problem.html#blands-rule"><i class="fa fa-check"></i><b>5.3</b> Bland’s rule<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="standardization.html"><a href="standardization.html"><i class="fa fa-check"></i><b>6</b> Standardization<span></span></a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="standardization.html"><a href="standardization.html#objective"><i class="fa fa-check"></i><b>6.0.1</b> Objective<span></span></a></li>
<li class="chapter" data-level="6.0.2" data-path="standardization.html"><a href="standardization.html#constraints"><i class="fa fa-check"></i><b>6.0.2</b> Constraints<span></span></a></li>
<li class="chapter" data-level="6.0.3" data-path="standardization.html"><a href="standardization.html#variables"><i class="fa fa-check"></i><b>6.0.3</b> Variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="dual-linear-program.html"><a href="dual-linear-program.html"><i class="fa fa-check"></i><b>7</b> Dual linear program<span></span></a></li>
<li class="chapter" data-level="8" data-path="weak-and-strong-duality.html"><a href="weak-and-strong-duality.html"><i class="fa fa-check"></i><b>8</b> Weak and strong duality<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="weak-and-strong-duality.html"><a href="weak-and-strong-duality.html#weak-duality"><i class="fa fa-check"></i><b>8.1</b> Weak duality<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="weak-and-strong-duality.html"><a href="weak-and-strong-duality.html#strong-duality"><i class="fa fa-check"></i><b>8.2</b> Strong duality<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="certificate-of-optimality.html"><a href="certificate-of-optimality.html"><i class="fa fa-check"></i><b>9</b> Certificate of optimality<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="certificate-of-optimality.html"><a href="certificate-of-optimality.html#complimentary-slackness"><i class="fa fa-check"></i><b>9.1</b> Complimentary slackness<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dual-of-a-general-linear-program.html"><a href="dual-of-a-general-linear-program.html"><i class="fa fa-check"></i><b>10</b> Dual of a general linear program<span></span></a></li>
<li class="chapter" data-level="11" data-path="sensitivity-analysis---constraints.html"><a href="sensitivity-analysis---constraints.html"><i class="fa fa-check"></i><b>11</b> Sensitivity analysis - Constraints<span></span></a>
<ul>
<li class="chapter" data-level="11.1" data-path="sensitivity-analysis---constraints.html"><a href="sensitivity-analysis---constraints.html#matrix-notation"><i class="fa fa-check"></i><b>11.1</b> Matrix notation<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="sensitivity-analysis---constraints.html"><a href="sensitivity-analysis---constraints.html#range-of-optimality"><i class="fa fa-check"></i><b>11.2</b> Range of optimality<span></span></a></li>
<li class="chapter" data-level="11.3" data-path="sensitivity-analysis---constraints.html"><a href="sensitivity-analysis---constraints.html#rate-of-change"><i class="fa fa-check"></i><b>11.3</b> Rate of change<span></span></a></li>
<li class="chapter" data-level="11.4" data-path="sensitivity-analysis---constraints.html"><a href="sensitivity-analysis---constraints.html#dual-variables"><i class="fa fa-check"></i><b>11.4</b> Dual variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sensitivity-analysis---objective.html"><a href="sensitivity-analysis---objective.html"><i class="fa fa-check"></i><b>12</b> Sensitivity analysis - Objective<span></span></a></li>
<li class="chapter" data-level="13" data-path="convexity.html"><a href="convexity.html"><i class="fa fa-check"></i><b>13</b> Convexity<span></span></a>
<ul>
<li class="chapter" data-level="13.1" data-path="convexity.html"><a href="convexity.html#convex-programming"><i class="fa fa-check"></i><b>13.1</b> Convex programming<span></span></a></li>
<li class="chapter" data-level="13.2" data-path="convexity.html"><a href="convexity.html#boundary"><i class="fa fa-check"></i><b>13.2</b> Boundary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="convexity-and-duality.html"><a href="convexity-and-duality.html"><i class="fa fa-check"></i><b>14</b> Convexity and duality<span></span></a>
<ul>
<li class="chapter" data-level="14.1" data-path="convexity-and-duality.html"><a href="convexity-and-duality.html#farkas-lemma"><i class="fa fa-check"></i><b>14.1</b> Farkas’ lemma<span></span></a></li>
<li class="chapter" data-level="14.2" data-path="convexity-and-duality.html"><a href="convexity-and-duality.html#geometry"><i class="fa fa-check"></i><b>14.2</b> Geometry<span></span></a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="convexity-and-duality.html"><a href="convexity-and-duality.html#convex-cones"><i class="fa fa-check"></i><b>14.2.1</b> Convex cones<span></span></a></li>
<li class="chapter" data-level="14.2.2" data-path="convexity-and-duality.html"><a href="convexity-and-duality.html#hyperplanes-and-half-spaces"><i class="fa fa-check"></i><b>14.2.2</b> Hyperplanes and Half-spaces<span></span></a></li>
<li class="chapter" data-level="14.2.3" data-path="convexity-and-duality.html"><a href="convexity-and-duality.html#geometric-version-of-farkas-lemma"><i class="fa fa-check"></i><b>14.2.3</b> Geometric version of Farkas’ lemma<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="convexity-and-duality.html"><a href="convexity-and-duality.html#convex-polyhedra"><i class="fa fa-check"></i><b>14.3</b> Convex polyhedra<span></span></a></li>
<li class="chapter" data-level="14.4" data-path="convexity-and-duality.html"><a href="convexity-and-duality.html#equivalence-with-strong-duality"><i class="fa fa-check"></i><b>14.4</b> Equivalence with Strong Duality<span></span></a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="interior-point-methods.html"><a href="interior-point-methods.html"><i class="fa fa-check"></i><b>15</b> Interior point methods<span></span></a>
<ul>
<li class="chapter" data-level="15.1" data-path="interior-point-methods.html"><a href="interior-point-methods.html#gradient-descent"><i class="fa fa-check"></i><b>15.1</b> Gradient descent<span></span></a></li>
<li class="chapter" data-level="15.2" data-path="interior-point-methods.html"><a href="interior-point-methods.html#barrier-functions"><i class="fa fa-check"></i><b>15.2</b> Barrier functions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="kkt-conditions.html"><a href="kkt-conditions.html"><i class="fa fa-check"></i><b>16</b> KKT conditions<span></span></a>
<ul>
<li class="chapter" data-level="16.1" data-path="kkt-conditions.html"><a href="kkt-conditions.html#kkt-conditions-1"><i class="fa fa-check"></i><b>16.1</b> KKT conditions<span></span></a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interior-point-methods" class="section level1 hasAnchor" number="15">
<h1><span class="header-section-number">Chapter 15</span> Interior point methods<a href="interior-point-methods.html#interior-point-methods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Throughout this chapter we will let <span class="math inline">\(f\)</span> denote a twice differentiable function from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{R}\)</span>.</p>
<div id="gradient-descent" class="section level2 hasAnchor" number="15.1">
<h2><span class="header-section-number">15.1</span> Gradient descent<a href="interior-point-methods.html#gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will start by trying to solve the following unconstrained optimization problem:
<span class="math display">\[\begin{align}
  \mbox{minimize: } &amp; f(x)
\end{align}\]</span>
where <span class="math inline">\(x\)</span> is any vector in <span class="math inline">\(\mathbb{R}^n\)</span>.
<strong>Gradient descent</strong> is a simple algorithm for solving this problem using basic differential calculus.
Gradient descent relies on the fact that <em>the negative of the gradient points in the direction in which the function decreases the fastest</em>.
So the general principle is to move in the direction of the negative gradient until the function is no longer decreasing.</p>
<p>More precisely,</p>
<ul>
<li><p>Let <span class="math inline">\(k = 0\)</span>.</p></li>
<li><p>Start with a guess <span class="math inline">\(x_0\)</span> for the optimal solution.</p></li>
<li><p>While minima is not reached:</p>
<ul>
<li>Set <span class="math inline">\(x_{k+1} = x_k - t_k \nabla f(x_k)\)</span>.</li>
<li>Increase <span class="math inline">\(k\)</span> to <span class="math inline">\(k+1\)</span>.</li>
</ul></li>
</ul>
<p>Here <span class="math inline">\(t_k\)</span> is the “step size” for the <span class="math inline">\(k^{th}\)</span> iteration and can be chosen to be a small constant or some function of <span class="math inline">\(k\)</span>, <span class="math inline">\(x_k\)</span>, <span class="math inline">\(f(x_k)\)</span>, or <span class="math inline">\(\nabla f(x_k)\)</span>.</p>
<p>There are several issues with the above algorithm:</p>
<ol style="list-style-type: decimal">
<li>If the function has multiple local minima, then the algorithm might converge to a non-absolute minima depending on the starting guess and the choice of step sizes.</li>
<li>If the step sizes are chosen to be too large, then the algorithm can completely miss the minima and may not converge.</li>
<li>If the step sizes are chosen to be too small, then the algorithm may take a long time to converge.</li>
</ol>
<p>Unfortunately, there is no easy way to resolve these issues. In practice, either we need to be able to control the function and its gradient or we proceed by trial and error to find step sizes that work. Even then there is no guarantee that the algorithm will converge to an absolute minima and not a local minima, if at all. In spite of these issues, because of its simplicity, ease of implementation, and good convergence properties in practice, Gradient Descent is a very popular algorithm for solving unconstrained optimization problems.</p>
</div>
<div id="barrier-functions" class="section level2 hasAnchor" number="15.2">
<h2><span class="header-section-number">15.2</span> Barrier functions<a href="interior-point-methods.html#barrier-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gradient descent algorithm can be modified to solve constrained optimization problems. Consider the following problem:</p>
<p><span class="math display" id="eq:constrained-optimization-problem">\[\begin{equation}
  \begin{array}{llr}
    \mbox{minimize: } &amp; f(x) \\
    \mbox{subject to: } &amp; g_i(x) \ge 0, &amp; \mbox{ for } 1 \le i \le m. \\
  \end{array}
  \tag{15.1}
\end{equation}\]</span></p>
<p>We can apply GD to this problem and find a critical point for <span class="math inline">\(f(x)\)</span>. However, this might not answer the optimization question for two reasons:</p>
<ol style="list-style-type: decimal">
<li>The critical point might not be in the feasible region.</li>
<li>The optimal solution might not be obtained at a critical of <span class="math inline">\(f(x)\)</span> and could lie on the boundary, as is the case for linear programming.</li>
</ol>
<p>The issue is that the gradient function algorithm only <em>sees</em> the objective function and does not <em>know</em> the constraints. So, we modify the objective function to include the constraints using barrier functions.</p>
<p>A <strong>barrier function</strong> is a differentiable function <span class="math inline">\(b\)</span> from <span class="math inline">\((0, \infty)\)</span> to <span class="math inline">\(\mathbb{R}\)</span> that has the property <span class="math inline">\(\lim \limits_{x \to 0^+} f(x) = \infty\)</span>. We’ll use the barrier function <span class="math inline">\(-\ln x\)</span>. Using a small positive parameter <span class="math inline">\(\mu\)</span>, we create a new objective function:</p>
<p><span class="math display">\[\begin{align}
  f_\mu(x) := &amp; f(x) - \mu \sum \limits_{i = 1} ^ m \ln (g_i(x)).
\end{align}\]</span></p>
<p>Because the domain of <span class="math inline">\(\ln(x)\)</span> is <span class="math inline">\((0, \infty)\)</span>, any critical point of <span class="math inline">\(f_\mu(x)\)</span> must lie in the feasible region of <a href="interior-point-methods.html#eq:constrained-optimization-problem">(15.1)</a>. Moreover, the following unconstrained optimization problem will always have an optimal solution away from the boundary of <a href="interior-point-methods.html#eq:constrained-optimization-problem">(15.1)</a> as the <em>cost</em> of approaching the boundary goes to <span class="math inline">\(\infty\)</span>:</p>
<p><span class="math display">\[\begin{align}
  \mbox{minimize: } &amp; f_\mu (x).
\end{align}\]</span></p>
<p>Let <span class="math inline">\(x_\mu^*\)</span> be a solution of the above problem. We’ll make the following assumption without proof: <em><span class="math inline">\(x_\mu^*\)</span> is continuous in <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\lim \limits_{\mu \to 0^+} x_\mu^* = x^*\)</span> where <span class="math inline">\(x^*\)</span> is an optimal solution to <a href="interior-point-methods.html#eq:constrained-optimization-problem">(15.1)</a>.</em> This assumption is valid, for example, when <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g_i(x)\)</span> are convex functions. With this assumption, we now have a method for solving <a href="interior-point-methods.html#eq:constrained-optimization-problem">(15.1)</a>:</p>
<ul>
<li>Start with a small <span class="math inline">\(\mu_0 &gt; 0\)</span> and optimize <span class="math inline">\(f_{\mu_0}(x)\)</span> using GD. Call the solution <span class="math inline">\(x_{\mu_0}^*\)</span>.</li>
<li>Repeat the following until the sequence <span class="math inline">\(x_{\mu_k}^*\)</span> stabilizes sufficiently:
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\mu_{k+1} = \mu_k - \delta_k\)</span> for some small <span class="math inline">\(\delta_k\)</span>.</li>
<li>Using <span class="math inline">\(x_{\mu_k}\)</span> as a starting point for GD, optimize <span class="math inline">\(f_{\mu_{k+1}}(x)\)</span>. Call the solution <span class="math inline">\(x_{\mu_k}^*\)</span>.</li>
</ol></li>
</ul>
<p>This method is a very simplified <strong>interior point method</strong>. The sequence of points <span class="math inline">\(x_{\mu}\)</span> is called the <strong>central path</strong>.</p>
<div class="example">
<p><span id="exm:ip-kkt" class="example"><strong>Example 15.1  </strong></span>We can compute the central path explicitly for some simple examples. Consider the optimization problem:</p>
<p><span class="math display">\[\begin{align}
  \mbox{minimize: } &amp; (x + 1)^2 \\
  \mbox{subject to: } &amp; x \ge 0.
\end{align}\]</span></p>
<p>It is easy to see that <span class="math inline">\(x^* = 0\)</span> is the optimal solution. We can calculate the central path as follows:</p>
<p><span class="math display">\[\begin{align}
  f_\mu(x) := (x + 1)^2 - \mu \ln (x).
\end{align}\]</span></p>
<p>The critical points for this function can be obtained by setting the derivative to 0.</p>
<p><span class="math display">\[\begin{align}
  f&#39;_\mu(x) &amp;= 0 \\ 
  \implies 2(x + 1) - \dfrac{\mu}{x} &amp;= 0 \\
  \implies x^2 + x - \mu/2 &amp;= 0 \\
  \implies x &amp;= \dfrac{-1 \pm \sqrt{1 + 2 \mu}}{2}.
\end{align}\]</span></p>
<p>Only one of the two satisfy <span class="math inline">\(x \ge 0\)</span>, so we get</p>
<p><span class="math display">\[\begin{align}
  x_\mu^* = \dfrac{-1 + \sqrt{1 + 2 \mu}}{2}. 
\end{align}\]</span></p>
<p>This is the central path. Taking the limit as <span class="math inline">\(\mu \to 0\)</span> we get,</p>
<p><span class="math display">\[\begin{align}
  \lim \limits_{\mu \to 0^+} x_\mu^* 
  &amp; = \lim \limits_{\mu \to 0^+} \dfrac{-1 + \sqrt{1 + 2 \mu}}{2} \\ 
  &amp;= 0 \\
  &amp;= x^*.
\end{align}\]</span></p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="convexity-and-duality.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="kkt-conditions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": "github"
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
