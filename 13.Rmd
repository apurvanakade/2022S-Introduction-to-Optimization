# Convexity

We will now move toward generalizing the concepts from linear programming to non-linear optimization questions. 
Broadly speaking, we have studied three different topics so far: 

1. Simplex method,
2. Duality theory, and 
3. Sensitivity analysis.

Of these the simplex method is hardest to generalize to non-linear optimization while sensitivity analysis is the easiest.^[Other techniques for solving linear programs like the ellipsoid algorithms and interior point methods *do* generalize to non-linear programs.] In fact, sensitivity analysis is important for any modeling question (not just optimization problems) and the general techniques for it lie beyond the scope of this course. This leaves us with duality theory. The class of optimization to which duality theory naturally extends is *convex optimization problems*.


## Convex programming

We start by defining convex sets, convex functions, closed sets, and convex optimization problems.

We say that a subset $\mathcal{S}$ of $\mathbb{R}^n$ is called **convex** if for any points $\vec{x}_1$ and $\vec{x}_2$ in $\mathcal{S}$, the line segment connecting them also lies in $\mathcal{S}$. More precisely, we say that $S$ is **convex** if forall $\vec{x}_1$ in $\mathcal{S}$, $\vec{x}_2$ in $\mathcal{S}$, and $t \in [0,1]$, the point $(1 - t) \vec{x}_1 + t \vec{x}_2$ also lies in $\mathcal{S}$.

We say that function $f : \mathcal{S} \to \mathbb{R}$ is **convex** if for all $\vec{x}_1$ in $\mathcal{S}$, $\vec{x}_2$ in $\mathcal{S}$, and $t \in [0,1]$, the inequality $f((1 - t) \vec{x}_1 + t \vec{x}_2) \le (1 - t) f(\vec{x}_1) + t f(\vec{x}_2)$ holds. Geometrically, this is saying that any line segment connecting two points on the graph of $f$ lies above the graph. 
The following theorem provides a quick way to check if a function is convex and to come up with examples of convex functions. 

::: {.theorem #convexity-second-derivative}

Let $f : \mathbb{R}^n \to \mathbb{R}$ be a $C^2$ function. $f$ is convex if and only if the Hessian matrix of $f$ is positive semi-definite. In particular, when $n = 1$, $f$ is convex if and only if $f'' \ge 0$.

:::


We say that a set is **closed** if contains all of its limit points. The precise definition of closedness is beyond the scope of this class. Geometrically, a set is closed if it contains its boundary. For example, the set $[0,1]$ is a closed subset of $\mathbb{R}^1$ whereas the set $[0,1)$ is not.

Finally, an optimization problem 
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize: } & f(x)  \\ 
    \mbox{subject to: } & x \in \mathcal{S} 
  \end{array}
  (\#eq:convex-optimization-problem)
\end{equation}
is called **convex** if $\mathcal{S}$ is a closed and convex subset of $\mathbb{R}^n$ and $f : \mathcal{S} \to \mathbb{R}$ is a convex function.

The following theorem is easy to prove using basic definitions.

::: {.theorem}

A linear program is a convex optimization problem.

:::

::: {.remark}

From now on, we'll let $\mathcal{S}$ denote a closed and convex subset of $\mathbb{R}^n$ and let $f$ denote a convex function from $\mathcal{S}$ to $\mathbb{R}$.

:::


## Boundary 


::: {.theorem #optimal-on-boundary}
The convex optimization problem in Equation \@ref(eq:convex-optimization-problem) either has an optimal solution on the boundary of $\mathcal{S}$ or is unbounded.

:::

::: {.proof}

Suppose $\vec{x}$ is a point in the interior of $\mathcal{S}$. 
It suffices to show that either there is some point on the boundary of $\mathcal{S}$ with an objective value that is greater than or equal to the objective value of $\vec{x}$ or the problem is unbounded.

Let $\vec{x}_1$ be any point on the boundary of $\mathcal{S}$. Draw a ray starting at $\vec{x}_1$ in the direction of $\vec{x}$. 
As $\mathcal{S}$ is closed and convex, two cases are possible: 

1. The ray intersects the boundary of $\mathcal{S}$ in exactly one more point, say $\vec{x}_2$.
2. The ray does not intersect the boundary of $\mathcal{S}$ and the entire ray is contained within $\mathcal{S}$.

**Case 1:** 

We will show that either $f(\vec{x}) \le f(\vec{x}_1)$ or $f(\vec{x}) \le f(\vec{x}_2)$. 
We prove this by contradiction. Suppose $f(\vec{x}) > f(\vec{x}_1)$ and $f(\vec{x}) > f(\vec{x}_2)$. 
Multiplying the first inequality by $(1-t)$ and the second by $t$ and adding them together, we get

\begin{align}
  && (1-t) f(\vec{x}) + t f(\vec{x}) &> (1-t) f(\vec{x}_1) + t f(\vec{x}_2) \\ 
  \implies 
  && f(\vec{x}) &> (1-t) f(\vec{x}_1) + t f(\vec{x}_2) .
\end{align}

But this contradicts the convexity of $f$.


**Case 2:**

If $f(x_1) \ge f(x)$, we're done. Suppose this is not the case. We'll show that the problem is unbounded. 

For $0 < t \le 1$, let $\vec{y}_t$ denote the point on the ray such that 

\begin{align}
  \vec{x} = (1-t) \vec{x}_1 + t \vec{y}_t.
\end{align}

For example, $\vec{y}_1 = \vec{x}$ and $\vec{y}_{1/2}$ is the point on the ray for which $\vec{x}$ is halfway between $\vec{x}_1$ and $\vec{y}_{1/2}$. As $t$ decreases, $\vec{y}_t$ will move farther and farther away from $\vec{x}$. Note that all of $\vec{y}_t$ lie inside $\mathcal{S}$. By convexity of $f$, we know that 

\begin{align}
  && f(\vec{x}) & \le (1-t) f(\vec{x}_1) + t f(\vec{y}_  t) \\
\implies && f(\vec{x}) - (1-t) f(\vec{x}_1) &\le t f(\vec{y}_t) \\
\implies && t^{-1}f(\vec{x}) - (t^{-1}-1) f(\vec{x}_1) &\le f(\vec{y}_t) \\
\implies && t^{-1}(f(\vec{x}) - f(\vec{x}_1)) + f(\vec{x}_1) &\le f(\vec{y}_t) \\
\end{align}

As $f(\vec{x}) > f(\vec{x}_1)$, the left hand side tends to $\infty$ as $t$ decreases and the optimization problem is unbounded.

:::

The following corollary follows by applying [Extreme Value Theorem](https://en.wikipedia.org/wiki/Extreme_value_theorem) to the above result.

::: {.theorem}

If $\mathcal{S}$ is bounded (in addition to being closed and convex), then the convex optimization problem in Equation \@ref(eq:convex-optimization-problem) has an optimal solution on the boundary of $\mathcal{S}$.

:::
