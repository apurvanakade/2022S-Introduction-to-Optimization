# Convexity

We will now move toward generalizing the concepts from linear programming to non-linear optimization questions. 
Broadly speaking, we have studied three different topics so far: 

1. Simplex method,
2. Duality theory, and 
3. Sensitivity analysis.

Of these the simplex method is hardest to generalize for any non-linear optimization problem while sensitivity is the easiest. In fact, sensitivity analysis is important for any modeling question and not just optimization problems and the general techniques for it lie beyond the scope of this course. As such, we'll focus on generalizing duality theory. The class of optimization for which some form of duality holds is the class of *convex optimization problems*.


## Convex programming

We say that a subset $\mathcal{S}$ of $\mathbb{R}^n$ is called **convex** if for any points $\vec{x}_1$ and $\vec{x}_2$ in $\mathcal{S}$, the line segment connecting them also lies in $\mathcal{S}$. More precisely, we say that $S$ is **convex** if forall $\vec{x}_1$ in $\mathcal{S}$, $\vec{x}_2$ in $\mathcal{S}$, and $t \in [0,1]$, the point $(1 - t) \vec{x}_1 + t \vec{x}_2$ also lies in $\mathcal{S}$.

From now on, we'll let $\mathcal{S}$ denote a convex subset of $\mathbb{R}^n$.

We say that function $f : \mathcal{S} \to \mathbb{R}$ is **convex** if for all $\vec{x}_1$ in $\mathcal{S}$, $\vec{x}_2$ in $\mathcal{S}$, and $t \in [0,1]$, the inequality $f((1 - t) \vec{x}_1 + t \vec{x}_2) \le (1 - t) f(\vec{x}_1) + t f(\vec{x}_2)$ holds. Geometrically, this is the same as saying that the line segment connecting the two points lies above the graph of $f$.

::: {.theorem #convexity-second-derivative}

Let $f : \mathbb{R}^n \to \mathbb{R}$ be a $C^2$ function. $f$ is convex if and only if the Hessian matrix of $f$ is positive semi-definite. In particular, when $n = 1$, $f$ is convex if and only if $f'' \ge 0$.

:::

This theorem provides a quick way to check if a function is convex. 

Finally, we say that a set is **closed** if contains all of its limit points. The precise definition of closedness is beyond the scope of this class. Geometrically, a set is closed if it contains its boundary. For example, the set $[0,1]$ is a closed subset of $\mathbb{R}^1$ whereas the set $[0,1)$ is not.

An optimization problem 
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize: } & f(x)  \\ 
    \mbox{subject to: } & x \in \mathcal{S} 
  \end{array}
  (\#eq:convex-optimization-problem)
\end{equation}
is called **convex** if $\mathcal{S}$ is a convex subset of $\mathbb{R}^n$ and $f : \mathcal{S} \to \mathbb{R}$ is a convex function.

The following theorem is easy to prove using basic definitions.

::: {.theorem}

A linear program is a convex optimization problem.

:::

In the next few chapters, we'll see how several results about linear programs generalize to convex optimization problems.

## Boundary 

Assume that $\mathcal{S}$ is a closed subset of $\mathbb{R}^n$.

::: {.theorem #optimal-on-boundary}
Consider the convex optimization problem in Equation \@ref(eq:convex-optimization-problem). Suppose that $\mathcal{S}$ is closed and bounded (i.e. compact). Then, the convex optimization problem has an optimal solution. Moreover, there is a point on the boundary of $\mathcal{S}$ where the optimal value is attained.

:::

::: {.proof}

The existence of an optimal solution is guaranteed by the [Extreme Value Theorem](https://en.wikipedia.org/wiki/Extreme_value_theorem).
Suppose $\vec{x}$ is a point in the interior of $\mathcal{S}$. 
It suffices to show that there is some point on the boundary with an objective value that is greater than or equal to the objective value of $\vec{x}$.


Let $\vec{x}_1$ be any point on the boundary of $\mathcal{S}$. Draw a ray starting at $\vec{x}_1$ in the direction of $\vec{x}$. 
As $\mathcal{S}$ is convex, closed, and bounded, the ray intersects the boundary of $\mathcal{S}$ in exactly one other point, call it $\vec{x}_1$. 
In this case, we need to show that either $f(\vec{x}) \le f(\vec{x}_1)$ or $f(\vec{x}) \le f(\vec{x}_2)$.
We'll prove this by contradiction. Suppose $f(\vec{x}) > f(\vec{x}_1)$ and $f(\vec{x}) > f(\vec{x}_2)$. 
Multiplying the first inequality by $(1-t)$ and the second by $t$ and adding them together, we get

\begin{align}
  && (1-t) f(\vec{x}) + t f(\vec{x}) &> (1-t) f(\vec{x}_1) + t f(\vec{x}_2) \\ 
  \implies 
  && f(\vec{x}) &> (1-t) f(\vec{x}_1) + t f(\vec{x}_2) .
\end{align}

But this contradicts the convexity of $f$.
:::